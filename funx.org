#+TITLE: A Functional Perspective on the NXP Architecture
#+SUBTITLE: Version {{{version}}}
#+AUTHOR: jmc
#+DATE: <2021-01-11 lun.>
#+OPTIONS: ':t toc:t author:t
#+LANGUAGE: en

#+MACRO: version 1.0

#+TEXINFO_FILENAME: funxp.info
#+TEXINFO_HEADER: @syncodeindex fn cp

#+TEXINFO_DIR_CATEGORY: NXP Architecture
#+TEXINFO_DIR_TITLE: funxp: (funxp)
#+TEXINFO_DIR_DESC: A Functional NXP Architecture

#+TEXINFO_PRINTED_TITLE: FUNXP

#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup

This manual is for FUNXP, a functional perspective on the NXP Architecture (version {{{version}}}).

* Introduction
Most of the ideas expressed in the historical series of computer programs, termed /NXP Architecture/ in this paper, were originally designed at the apex of one of the recurring return in the 30-year cycle in the Artificial Intelligence (AI) debate, @@texinfo:@xref{JMC2018,,1}.@@ 

** A Dominant View, Symbolic Artificial Intelligence
#+CINDEX: Symbolic AI
Progressively in the late 70s and early 80s, the symbolic thread triumphed as the dominant paradigm of AI research. In 1984 for instance, at the climax of this cycle, early efforts to transfer research work to industrial applications were so introduced:

#+BEGIN_QUOTE
Artificial Intelligence is the subfield of computer science concerned with symbolic reasoning and problem-solving. [...] Knowledge Engineering is the process of incorporating symbolic knowledge into computer systems to solve problems normally requiring human attention and intelligence.[fn:1]
#+END_QUOTE 

#+CINDEX: Expert System
These computer systems incorporating symbolic knowledge became known as /Expert Systems/ and were all the rage, as industrial applications of AI, from the early eighties to the mid-nineties.

#+CINDEX: Dartmouth
This somewhat restricted, to contemporary eyes, view of AI as a "subfield of computer science" geared towards augmenting traditional programs with symbolic knowledge was, however, perfectly in line with the precepts of the symbolic thread of AI research, established at the /Dartmouth Summer Research Project on Artificial Intelligence/ in 1956. Out of the variety of research theses on "thinking machines" in the 1950s, diversely known as /cybernetics/, /automata theory/ or /complex information processing/, stemmed out the original characteristics of the symbolic approach to AI: the importance of (mathematical) logic in high-level cognitive processes, and the prevalence of symbols in all aspects of cognition.

#+CINDEX: Physical Symbol System Hypothesis
The defining assumption, /the physical symbol system hypothesis/, was the philosophical perspective on AI assessed by Allen Newell and Herbert A. Simon:

#+BEGIN_QUOTE
A physical symbol system has the necessary and sufficient means for general intelligent action, @@texinfo:@xref{Newell1972,,3}.@@
#+END_QUOTE 

#+CINDEX: goal
#+CINDEX: working memory
#+CINDEX: computer/brain analogy
#+CINDEX: Dartmouth
With symbols, structures come naturally as list of symbols, and cognitive processes are understood as controlled by signal and symbol structures in /working memory/, i.e. arbitrary list structures in an extensive database-like system, sometimes called /goals/, @@texinfo:@xref{PDIS1978,,4}.@@ On the one hand, note the implicit analogy with the Turing machine model of a computer[fn:2], where perceptions are turned into symbols in memory, feeding a processor which elects further actions, operating changes in the environment. And, on the other hand, delineating further this analogy, the notions of symbol and list from which LISP, and all its functional programming languages descendants in computer science, was derived, and moreover by the same contributors to the seminal Dartmouth workshop[fn:3].

#+CINDEX: CMU
While the seminal ideas for the NXP Architecture stemmed from a very early interest in what would be now termed /learning concepts/ (see [[How we got here]]), they were readily influenced and refined by the dominant view at the time, all the more so that they matured in the cultural environment of Carnegie-Mellon University (CMU), namely of its Computer Science and Robotics Institute departments, from 1982 to 1984.

** Production Systems and Rule-based Systems
#+CINDEX: rule-based system
#+CINDEX: rule system
#+CINDEX: production system
With the symbolic AI view then firmly entrenched, both in computer science and in the emerging philosophy of the mind, the idea of rule-based systems, in which /knowledge/ is conventionally represented as /rules/ operating on symbols (following the Physical Symbol System hypothesis) became an active applied research program.

#+CAPTION: Model Human Processor in /The Psychology of Human-computer Interaction/, by Card, Newell, Moran (1983). The brain-as-a-computer metaphor in the philosophy of the mind, all quantified!
#+attr_texinfo: :width 200px :center t
[[./MODEL-HUMAN-PROCESSOR-w200.jpg]]

Although rule-based computation was originally used for formal and systems purposes , researchers in Artificial Intelligence (AI) found that the same methodology was also useful for modeling a wide variety of sophisticated tasks.

#+BEGIN_QUOTE
The production system was one of those happy events, though in minor key, that historians of science often talk about: a rather well-prepared formalism, sitting in wait for a scientific mission. Production systems have a long and diverse history. Their use in symbolic logic starts with Post, @@texinfo:@xref{Post1943,,7},@@ from whom the name is taken. They also show up as Markov algorithms, @@texinfo:@xref{Markov1957,,8}.@@ Their use in linguistics, where they are also called rewrite rules, dates from Chomsky, @@texinfo:@xref{Chomsky57,,10}.@@ As with many other notions in computer science, they really entered into wide currency when they became operationalized in programming languages, first in string manipulation systems and in compiler translation languages, @@texinfo:@xref{Floyd1961,,9}.@@ Thus they were at hand when the data on human problem solving finally took a form (Problem Behavior Graphs) that pointed to their usefulness, @@texinfo:@xref{Newell1972,,3}.@@
#+END_QUOTE

As commented on by Lenat[fn:4], there were many design constraints present in the classical formal rule based systems. Many of these details were preserved in the AI production rule based programs (e.g., forcing all state information into a single string of tokens). But there were also many changes. The whole notion of "what a rule system really is" changed from an effective problem statement to a tendency to solve problems in a particular way. One typical corollary of this change of view was that instead of no external inputs whatsoever, there was now a presumption of some "environment" which supplied new entries into the token sequence.

#+CINDEX: rule-based-system
#+CINDEX: working memory
#+CINDEX: inference engine
#+CINDEX: recognize-act-cycle
So over the 1970s symbolic AI research mostly worked with rule systems (RS), a collection of condition-action rules, together with associated data structures (DS; also called memories, or /working memory/) which the rules may inspect and alter. There must also be a policy for /interpretation/: detecting and firing relevant rules (also known as /recognize-act-cycle/, later called /inference engine/). "Intelligence is ten million rules," [[https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/][Lenat pronounced in 1988]], such were the times.

*** Neo-classical Rule System Architecture
#+CINDEX: rule system
#+CINDEX: LHS
#+CINDEX: RHS
#+CINDEX: working memory
#+CINDEX: conditions
#+CINDEX: actions
In ten loose principles according to Lenat and Harris:

#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
- Principle of Simple Memories :: One or two uniform data structures define sufficient memories for a rule system to read from and write into. The format for entries In these structures is both uncomplicated and unchanging.
- Principle of Simple DS Accesses :: The primitive read and write operations are as simple and low-level as possible; typically they are simply a membership or equality test type of read, and an insert-new-element or set-value type of write. More complicated, algorithmic operations on the memories are not available to the rules.
- Principle of Isolated DS Elements :: Elements of the uniform DS cannot point to (parts of) other elements. This follows from the preceding principle: If we aren't allowed to chase pointers, there may as well not be any.
- Principle of Continuous Attention :: In addition to the one or two simple data structures, there may be an external environment which continuously inserts stimuli into the DS. The interleaving of stimuli and internally generated symbols is managed quite trivially: (a) The stimuli are simply inserted into the DS as new or changed elements; (b) Each rule is so small and quick that no "interruption" mechanism is necessary. The interpreter may ignore any suddenly-added stimulus until the current rule finishes executing. The RS may be viewed as "continuously" attending to the environment.
- Principle of Opaque Rules :: Rules need not have a format inspectable by other rules, but rather can be coded in whatever way is convenient for the programmer and the rule interpreter; i.e., the set of rules is not treated as one of the RSs data structures. E.g., the condition parts of rules may be barred from fully analyzing the set of productions, and the action parts of rules may not be allowed to operate on existing rules.
- Principle of Simple Rules :: Rules consist of a left- and a right-hand side which are quite elementary. The left hand side (lhs, situation characterization, IF-part, condition) is typically a pattern-match composed with a primitive DS read access, and the right hand side (rhs, consequence, THEN-part, action) is also simply a primitive DS write access. There is no need for sophisticated bundles of DS accesses on either side of a rule. Thus several extra rules should be preferred to a single rule with several actions.
- Principle of Encoding by Coupled Rules :: A collection of interrelated rules is used to accomplish each subtask; i.e., wherever a subroutine would be used in a procedural programming language. For example, programming an iteration may require many rules "coupled" by writing and reading special (Le., otherwise meaningless) loop control notes in the data structure. 
-  Principle of Knowledge as Rules :: All knowledge of substance should be, can : be, and is represented as rules. This includes all non-trivial domain dependent information. The role of the DS is just to hold simple descriptive information, intermediate control state messages, recent stimuli from the environment, etc.
- Principle of Simple Interpretation :: The topmost control flow in the RS is via a simple rule interpreter. After a rule fires, it is essential that any rule in the system may potentially be the next one to fire (i.e., it is forbidden to locate a set of relevant rules and fire them off in sequence). When the rhs of a rule is executed, it can (and frequently will) drastically alter the situation that determined which rules were relevant.
- Principle of Closure :: The representations allowed by (1-9) are sufficient and appropriate for organizing all the kinds of knowledge needed for tasks for which a given RS is designed.

Notice the common theme: the adequacy of simplicity in all dimensions. 

Medical consultation as a task environment.

*** Rules are put to many uses in inference
In contrast, defending that thinking may be more than computing[fn:5], Peter Kugel refers to Peirce's definitions of:

#+CINDEX: rule system
#+CINDEX: rule
#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
- Rule :: a general principle that is applied to specific examples. The analogy to a program representing an principle that computers apply to certain inputs still stands.
- Case :: what the rule is applied to. This would be the input in the computer program metaphor, or the working memory in the rule system.
- Result :: what is produced by the rule application. This would be the output in the computer program metaphor, or the effects of RHS actions on the working memory in a rule system.

#+CINDEX: deduction
#+CINDEX: induction
#+CINDEX: abduction
#+CINDEX: machine learning
The overall analogy suggests that deduction might be modeled as evaluating a rule against case, which was often done at that time. While in logic, axiomatic theories are often thought as recursively enumerable (i.e. partially computable) theorems, here it is suggested that induction works from case and result to rule. And indeed, in the heydays of symbolic AI, Machine Learning research went this way, @@texinfo:@xref{Michalski1984,,29}, @xref{Michalski1986,,30},@@ years before the massive connectionist architecture of today's ML took up the prize.

|            | DEDUCTION                      |
|------------+--------------------------------|
| Given:     | Rule: All men are mortal       |
| Given:     | Case: (SOCRATES IS-A MAN)      |
|------------+--------------------------------|
| Concludes: | Result: (SOCRATES IS-A MORTAL) |


|            | INDUCTION                      |
|------------+--------------------------------|
| Given:     | Case: (SOCRATES IS-A MAN)      |
| Given:     | Result: (SOCRATES IS-A MORTAL) |
|------------+--------------------------------|
| Concludes: | Rule: All men are mortal       |


|            | ABDUCTION                      |
|------------+--------------------------------|
| Given:     | Rule: All men are mortal       |
| Given:     | Result: (SOCRATES IS-A MORTAL) |
|------------+--------------------------------|
| Concludes: | Case: (SOCRATES IS-A MAN)      |

#+CINDEX: forward-chaining
#+CINDEX: backward-chaining
Peirce's types of inference are related, but not identical to, notions of /forward-chaining/ and /backward-chaining/ in rule systems, which are later explored in the FUNXP architecture.

#+CINDEX: CMU
#+CINDEX: DEC
#+CINDEX: VAX
#+CINDEX: OPS 5
In the CMU culture, at the time, the "neo-classical" view defined by Lenat was prevalent. Its incarnation in the series of production systems languages, OPS, culminated in OPS 5 (and later OPS 83). OPS 5 was made instantly famous by the then well-known significant success of a first industrial application: R1/XCON, an expert system to configure VAX Systems at DEC[fn:6].

#+CAPTION: The OPS series of production systems languages. Source: Wikipedia, CC0, https://en.wikipedia.org/w/index.php?curid=44903117
#+attr_texinfo: :width 400px :center t
[[./OPS_series.png]]

#+CINDEX: RETE
#+CINDEX: recognize-act-cycle
In OPS-based expert systems, creation/update/deletion operations on the working memory were propagated into a graph, compiled from the rules. These changes triggered LHSes, concurrently selecting rules which could be fired at each step. The recognize-act-cycle parameters would drive picking up the rule(s) to fire, executing their RHSes and cycling back to rule selection[fn:7].

The numerous active developments, at CMU, about and around OPS 5 added to the design mix of the NXP Architecture. They helped contrasting and focusing on the proper interaction of backward chaining and forward chaining, that guide the logical process of rule evaluation.

*** Rules with and without symbols
#+CINDEX: neurosciences
#+CINDEX: cognitive control
Interestingly, the formidable development of neurosciences and computational neurosciences in the last decades throws a new contemporary light on the venerable rule system thread of symbolic AI. Based on mathematical-logic and computer-science inspired metaphors, the rule system ended up as a model of a fundamental human cognitive faculty: the capacity for cognitive control, the ability to behave in accord with rules, goals or intentions (all problematic notions to the philosophy of mind, by the way) -- even when this runs counter to reflexive or other compelling competing responses.

#+CINDEX: SOAR
A hallmark of this cognitive control is its remarkable flexibility. Novel tasks can be performed with very little additional experience (a problematic issue, in contrast, for the current -- 2021 -- crop of massive Deep Learning connectionist architectures). This was unfalteringly explored over several decades by, among others, Allen Newell, @@texinfo:@xref{Newell1972,,3},@@ then @@texinfo:@xref{Newell1990,,31}.@@

Today's neurosciences tell us that this ability appears to depend on the prefrontal cortex (PFC). This capacity, however, emerges only slowly over a protracted period through late adolescence. The rule system abstractly models flexible cognitive control at the psychological level, in terms of symbol processing computations that all support arbitrary variable bindings. A symbol may stand for anything, as computations only rely on the syntactic properties of such symbols[fn:3]. It remains unclear, however, whether or how this model relates to the increasingly growing body of knowledge about the neural mechanisms underlying cognitive control and namely the functioning of the PFC.

#+CINDEX: connectionism
#+CINDEX: connectionism (proper treatment)
At the biological level, many models were developed of cognitive control relying on the maintenance of rule-like representations in the PFC. But questions about how these representations develop and why this development should be so long are still unanswered, @@texinfo:@xref{Rougier2005,,2}.@@ Leveraging today's successes in massive connectionism, as is apparent in Deep Learning, neural networks models of the PFC can be trained to show development of rule-like task representations, which support generalization of task performance to novel environments. A perfect, modern instance of the proper treatment of connectionism, the approach pioneered more than thirty years ago that sought to set up "check and balances" strategies against an all-encompassing symbolic AI, @@texinfo:@xref{Smolensky1988,,32}.@@

** Clinical Consultation as a Task Environment
The third historical ingredient in the context for the original design in the NXP Architectures were some seminal applications of symbolic AI to Medicine, and more specifically to clinical consultation.

*** Old and Long History of Medical Applications of AI
#+CINDEX: AIM
In 1956, Dr François Paycha presented a paper at the /Premier Congrès International De Cybernétique/, in Namur, on "Cybernétique de la consultation", @@texinfo:@xref{Paycha1963,,11}.@@

#+CINDEX: sign
#+CINDEX: syndrome
#+CINDEX: symptom
#+CINDEX: clinical picture
Inspired by the Weaver-Shannon information theory and Norbert Wiener's Cybernetics, "Consultation Cybernetics" is a full-fledged theory of heuristics in medical diagnosis. Useful abstractions in the logical process of reaching a positive diagnosis are precisely defined in an inclusive sequence of sets of signs/symptoms, syndrome, clinical pictures, and finally diseases, joined by typed links, which may be directed or not. The "working memory" was then represented as a graph:

#+CAPTION: Medical Consultation: a view from cybernetician Dr François Paycha, in 1956.
#+Attr_texinfo: :width 400px :center t
[[./PAYCHA.jpg]]

Paycha suggests that "diagnosis machines" are needed to confront the increasingly growing volume of medical knowledge, already escaping the MD's comprehension. And diagnosis being only the first in a series of steps leading to prescription, and a healthy patient, the need is amplified by the increasing volume of therapeutics information.

The graph is traversed by the consultation exploratory process in three consecutive phases, termed /semiological/, /differential/, and /positive/ diagnosis, respectively. Based on a form of ternary logic, /true/, /false/, and /unknown/, the first two phases range from signs/symptoms up to candidate diseases through syndromes and their related clinical pictures. Then a directed ternary-logic evaluation of the candidates is performed in the last phase, reaching one, or possibly several, positive diagnoses.

#+CINDEX: goal
#+CINDEX: forward-chaining
#+CINDEX: backward-chaining
The theory articulates a data-driven phase, reminiscent of or heralding forward-chaining in rule systems, which Paycha insists is almost reflexive in the MD's mind -- thus also evocative of so-called spontaneous computations models studied twenty years later in computer science, @@texinfo:@xref{Rieger77,,6},@@ -- with a form of deductive backward-chaining goal and subgoal evaluation.

All the basic building blocks of AI in Medicine (AIM) were in place.

*** There Were Many, Many Inspiring Accomplishments in AIM
The early ground-breaking AIM systems were developed by inpired pioneers at Stanford, Pittsburgh University, Rutgers, MIT and a few other innovative centers of AI research in the seventies. These were designed by collaborative teams where computer scientists, MDs and hospitals all worked together, against a background of more traditional approaches to the use of computer in clinical medicine, @@texinfo:@xref{Szolovits1982,,33},@@:

- Flowchart :: or /clinical algorithm/ which encodes, in principle, the sequences of actions a good clinician would perform for any one of some population of patients.
- Data Bases :: (in two separate words, at the time) enabling, when data is properly captured, the /matching/ of cases to a large body of previous cases.
- Application of Decision Theory :: a mathematical theory of decision making under uncertainty, rooted in operations research.

which were all thought as lacking the flexibility required for full-fledged /usable/ clinical systems.

This crop of first generation programs proved over time to be inspiring accomplishments for later research:

#+CINDEX: CASNET
#+CINDEX: EXPERT
- CASNET :: developed at Rutgers University by Kulikowski, in its major instanciation as a diagnostic and therapeutic program for glaucoma and related diseases of the eye. EXPERT, a somewhat simpler descendant was more widely applied mostly in the analysis of thyroid disorders and rheumatology. It identified the fundamental issue of /causality/ as essential in the diagnostic and used /causal-associational networks/ as the core knowledge representation.
#+CINDEX: MYCIN
- MYCIN :: or as described by their developers Buchanan and Shortliffe, the series of MYCIN Experiments, at Stanford University, originally applied in the diagnosis and treatment of bacterial infections of the blood. Rules famously represent knowledge in MYCIN, often associated with so called /certainty factors/.
#+CINDEX: Digitalis Therapy Advisor
- Digitalis Therapy Advisor :: developed at MIT by Szolovits /et al./, where knowledge is captured in Minsky's /frames/ model he suggested for memory structures.
#+CINDEX: INTERNIST
#+CINDEX: CADUCEUS
- INTERNIST :: developed at the University of Pittsburgh by Pople, an ambitious program for diagnosis in general internal medicine. Special emphasis on the distinction between problem formulation and problem solving, a characteristic of this system, was further refined in CADUCEUS, a follow-on system in the mid-80s.
#+CINDEX: PIP (Present Illness Program)
#+CINDEX: ABEL
- PIP :: designed, at MIT, to emulate clinicians in the evaluation of patients with renal disease. It merged facts about the patient with knowledge from a database to develop a hypothesis about what was afflicting the patient. More complex cases were addressed by ABEL (by Patil), a program for the diagnosis, and eventual treatment, of acid/base and electrolyte disturbances.

#+CINDEX: MYCIN
Started in 1972 at Stanford University, MYCIN is a pioneering computer-based consultation system designed to assist physicians in the diagnosis of and therapy selection for patients with bacterial infections. In addition to the consultation system itself, MYCIN contains an explanation system which can answer simple English questions in order to justify its advice or educate the user. The system's knowledge is encoded in the form of some 350 production rules which embody the clinical decision criteria of infectious disease experts. Much of MYCIN's power derives from the modular, highly stylized nature of these decision rules, enabling the system to dissect its own reasoning and allowing easy modification of the knowledge base, @@texinfo:@xref{Buchanan1984,,14}.@@

#+CAPTION: The MYCIN Experiments at Stanford's SUMEX-AIM.
#+attr_texinfo: :width 300px :center t
[[./MYCIN.jpg]]

#+CINDEX: MYCIN
#+CINDEX: EMYCIN
#+CINDEX: ONCOCIN
#+CINDEX: expert system shell
Stanford's MYCIN and its descendants, such as EMYCIN[fn:9], a domain independent version of MYCIN for use in other domain and applications, and ONCOCIN[fn:8], an oncology protocol management system designed to assist physicians in the treatment of cancer patients, had a long-lasting influence on the whole program of AI research and precipitated the commercial the early eighties charge towards industrial and commercial applications of expert systems.

#+CINDEX: MYCIN
#+CINDEX: OPS
#+CINDEX: expert system shell
#+CINDEX: knowledge engineering
Predominantly based on backward-chaining, in contrast to the design of CMU's OPS series of production system languages, MYCIN nonetheless fixed the major features and definitions of expert systems, and /expert system shells/ as domain-independent /knowledge engineering/ software tools.

In 1983, Ed Shortliffe reflected on the impact of MYCIN on AI:

#+BEGIN_QUOTE 
You mentioned earlier that MYCIN is often cited as sort of the fundamental expert system -- which may be overstating the case for MYCIN. DENDRAL is certainly the earliest really well-accepted expert system, although it has some different elements . It's not interactive for large groups of people in quite the same way that MYCIN is intended to be . But ideas that grew out of MYCIN and
DENDRAL and other medical efforts such as the CASNET project at Rutgers, the INTERNIST project at the University of Pittsburgh, work at MIT on a program called PIP (the Present Illness Program), and subsequent work at MIT on explanation and more recently causal reasoning and multi-level causal descriptions, have contributed greatly to the state of the art in expert systems and, in turn, to the demonstration of these ideas.
#+END_QUOTE

#+CINDEX: rule interpreter
#+CINDEX: inference engine
#+CINDEX: rule compiler
#+CINDEX: certainty factors
#+CINDEX: interactivity
#+CINDEX: explanation
MYCIN made popular constructs like /rule interpreters/ (shells) and /rule compilers/ (in this case to decision trees) and /certainty factors/; main control structure as goal-directed backward chaining of rules[fn:10]; interactivity by asking questions to the user, modularity of rules which could be edited individually; explanation capabilities. (Note that early expert systems faced the same critic, which are today addressed to ML Deep Learning neural nets, of demanding /explanation/ for the conclusions it reached in order to be accepted as a legitimate tool by a community of practitioners.)

#+CAPTION: The Expert System Architecture industry standard in the late eighties.
#+attr_texinfo: :width 300px :center t
[[./EXPERT-SYSTEM.jpg]]


Its influence was certainly important on the NXP Architecture as its first demonstration knowledge bases, for research purposes, were all from the medical consultation task environment @@texinfo:@xref{Rappaport-1984-15190,,12}.@@ Closer to the CMU campus, INTERNIST-I was a broad-based computer-assisted diagnostic tool developed in the early 1970s at the University of Pittsburgh as an educational experiment[fn:11].

In hindsight, the NXP Architecture design strongly benefitted from the accomplishments of this opening chapter of AIM, @@texinfo:@xref{Szolovits1982,,33},@@[fn:24] @@texinfo:@xref{Clancey1984,,13}.@@ These stepping stones were retrospectively described in 2015 by Pr. Casimir Kulikowski, whose doctoral dissertation, in 1970, described a pattern recognition model which has been successfully used to simulate a doctor's diagnostic process:

#+BEGIN_QUOTE 
During the first half of the 1970’s, several groups working on computational models for clinical decision-making and problem-solving had developed the MYCIN rule-based system for infectious disease therapy assistance at Stanford, the CASNET Causal Associational NETwork model for consultation in glaucoma, at Rutgers, the DIALOG (later renamed INTERNIST) system for differential diagnosis in internal medicine at Pittsburgh, and the PIP (Present Illness Program)
for diagnosis-driven acquisition of clinical data at MIT and Tufts. These had been inspired by AI approaches that departed from the earlier general problem solving search paradigm characteristic of AI since its inception and still holding sway into the 1970’s, and focused on capturing domain- and problem-specific strategies for solving complex sequences of expert biomedical interpretations and actions. These included the rule-based and hypothesis-list approaches used in the DENDRAL Project, which influenced MYCIN, as well as
experimental, instructional, interview-based, and cognitive approaches to the analysis of clinical problem solving, and the causal-taxonomic representation of underlying processes of disease.  While earlier computer models for medical decision-making were predominantly statistical or algorithmic, the new AI approaches developed structured representations of specific clinical domain knowledge over which a general inference engine could reason with a variety of heuristics, and provide advice or suggestions to the consulting user.
#+END_QUOTE

#+CAPTION: Strategies in CASNET glaucoma consulation program, developed at Rutgers (1971-8) based on /causal association networks/.
#+attr_texinfo: :width 200px :center t
[[./CASNET.jpg]]

Closing the loop with the early cybernetics endeavours in Medicine of Dr Paycha, the state of the art of AI systems for medical consultation was summarized in computer programs such as NEOMYCIN. Revisiting the "MYCIN experiments" over close to twenty years, and reconfiguring the system for education purposes -- the ultimate use INTERNIST-I was also put to at about the same time -- the following diagram was published:

#+CAPTION: Strategies in MYCIN-inspired medical consultation computer programs.
#+attr_texinfo: :width 200px :center t
[[./MYCIN-STRATEGY.jpg]]

Note the similarities with Dr Paycha's investigations of 1956, allowing for changes in terminology and some refinements on the pedagogical aims of the classification.

* How we got here
The functional perspective motivates revisiting -- once more -- the design of the NXP Architecture. This section elaborates on what is expected of functionalism (as in the philosophy of mind) or of functional languages when it comes to the nature and implementation of the NXP Architecture.

** NXP Archeology and vestigial software artefacts
The seminal ideas, presented in context in the [[* Introduction]] slowly matured over time. Several computer programs, designed as scientific experiments on a computer model of the mind, in typical Newell-Simon style, testify to the long trek in the design space that brought us here and now.

#+CINDEX: differetential diagnosis
#+CINDEX: inference engine
#+CINDEX: clustering
#+CINDEX: NEXPERT
#+CINDEX: AIM
#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
- PHILIPS :: Associative memory; exploration of clustering in Machine Learning (1979-80).
- NClose :: An AIM inference engine with a focus on differential diagnosis in medical consultation (1982-3). Also Rappaport, A., /Closed Search: an hypothesis evaluator/, Unpublished manuscript, Robotics Institute, Carnegie-Mellon University, (1982).
- KAA :: Derived from PHILIPS, ML by clustering on the inference paths of the NClose performances (1982-3).
- SCS :: A knowledge management tool for the KAS/PROSPECTOR rule model, written in OPS 5 (1982-3). Also Mulsant, B., Servan-Schreiber, D. /A Gentle Introduction to Artificial Intelligence in Medicine/. Unpublished manuscript, Robotics Institute, Carnegie-Mellon University, (1982).
- PROSYL :: Nclose-inspired algorithm for many-objects many-patterns matching à la RETE (1983).
- AMBER :: Generalizing previous programs: discovery and clustering of sequential observations for prediction (1984).
- NEXPERT, later NEXPERT OBJECT :: Industry-standard expert system development environment on microcomputers, workstations and mainframes, with pioneering GUI (1985-)

Recent, and mostly unpublished, research work explored further implementations and design ideas about the NXP Architecture:

#+CINDEX: CPS
#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
- Theoretical models :: Expressing heuristics in continuation passing style (CPS) [[https://arxiv.org/abs/cs/0211035][Monadic Style Control Constructs for Inference Systems]] (2002), [[https://arxiv.org/abs/cs/0402035][Memory As A Monadic Control Construct In Problem-Solving]] (2004).
- NClosEmacs :: A Nclose-inspired rule/hypothesis evaluator written in Emacs-Lisp with a Machine Learning (ML) extension exploring /bagging/ and /boosting/ (2008-10).
- LLVM-based implementation :: Based on CLANG, CouchDB and a minimal GUI client in GTK+, a NXP Architecture based on CPS constructs (2010).
- Micro-Service implementation :: Written in Javascript and based on Node, Moleculer, the progressive microservice framework, a demonstration prototype with a React GUI (2018) for the Web.

** Meanwhile functionalism thrives in the philosophy of mind
#+CINDEX: functionalism
#+CINDEX: mental states
In the philosophy of mind, functionalism is generally considered one of the major proposals that have been offered as solutions to the mind/body problem[fn:12].  Functionalism says that mental states are constituted by their causal relations to one another and to sensory inputs and behavioral outputs.  
Functionalism is one of the major theoretical developments of Twentieth Century analytic philosophy, and provides the conceptual underpinnings of much work in cognitive science.

Ned Block lists three sources for functionalism:

#+BEGIN_QUOTE
  - Putnam and Fodor saw mental states in terms of an empirical computational theory of the mind[fn:3]. (A critical assumption in the symbolic v. connectionism recurring debate, where both sides appeal to a form of computation.)
  - Smart's "topic neutral" analyses led Armstrong and Lewis to a functionalist analysis of mental concepts. (In which /causality/ is emphasised[fn:13].)
  - Wittgenstein's idea of /meaning as use/ led to a version of functionalism as a theory of meaning, further developed by Sellars and later Harman.
#+END_QUOTE

Leveraging the mind-as-computer metaphor (see [[Rules with and without symbols]]), Block introduces the function relevant to mind by way of the Turing machine. What is a state of a Turing machine? Its nature is entirely relational: the state is completely defined by its relation with other states in the transition table. And so:

  * According to functionalism, the nature of a mental state is just like the nature of an automaton state: constituted by its relations to other states and to inputs and outputs;
  * Hence mental states can be totally characterized in terms that involve only logico-mathematical language and terms for input signals and behavioral outputs. (A syntactic view of characterizing mental processes.)
  * Mental states, howver, do have /other/ (e.g. physical) properties. These other properties are said to be the /realizations/ of the functional properties. So, although functionalism characterizes the mental in non-mental terms, it does so only by quantifying over realizations of mental states (which would not have delighted behaviorists).
  * Of course then, one functional state can be realized in different ways. (This, we saw, is a principle in the symbolic AI approach which abstracts the underlying substrate for mental processes.)
  * Conversely, one physical state can realize different functional states in different machines, including the brain.

#+CINDEX: physicalism
#+CINDEX: cognitive science
#+CINDEX: functionalism (conceptual)
#+CINDEX: functionalism (psychofunctionalism)
Functionalism then permeates cognitive science and AI[fn:14] since inception. It hints at the the falsity of physicalism:  if a creature without a brain can think, thinking can't be a brain state[fn:5].

The issue of realization is not without difficulty, however. Should we consider it in empirical psychology and functionalism aims at capturing mental concepts (as ordinarily understood); should we consider it in common sense psychology, and functionalism aims rather at fixing the extension of mental terms. (Reminiscent of distinctly--and non equivalently--defining a function in the mathematical sense, by extension, or by intension.) These variants of functionalism in turn spell different problems in relating mental states to causation, or to /qualia/ (phenomenal states like the look of /red/)[fn:15].

** And functional programming languages in computer science
#+CINDEX: LISP
#+CINDEX: Dartmouth
As mentioned before, the roots of the symbolic AI and functional programming languages are historically intertwined, dating back to the Dartmouth workshop of 1956. And so, at different times in its history, AI was just equated with LISP programs and LISP-Machines, LISP being the original and archetype functional programming language. In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. (In this respect, it shares some principles with versions of functionalism in the philosophy of mind glossed over in [[Meanwhile functionalism thrives in the philosophy of mind][previous section]].) 


It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value, rather than a sequence of imperative statements which change the state of the program. In this respect, it naturally blends with declarative knowledge, [[Production Systems and Rule-based Systems][rule systems]].


As an exploration path for this research project, we planned to revisit this blending of symbolic processe perspectives, viewed as mental processes and viewed as computer programs expressed in functional languages, leading to an original (re)implementation of the NXP Architecture.

*** Evaluation, Compilation and Abstract Machines
A solid trend in programming language design has been the use of operational semantics to define the semantics of a programming language. In this respect, Abstract machines have been effectively used as intermediate and low-level architectures suitable for supporting serious implementations of a wide variety of programming languages, including imperative, functional, and logic programming languages. Abstract machines are distinguished from operational
semantics by having simple and direct algorithmic implementations that can employ efficient data structures. On the other hand, abstract machines are distinguished from lower-level, machine-code implementations because typically the former uses pattern matching for manipulating data while the latter explicitly addresses the notions of structure sharing, storage allocation, and register allocation[fn:16]. 

#+CINDEX: abstract machine
#+CINCEX: Emacs-Lisp (implementation language)
In the above characterization of abstract machines their use as an intermediate language for compilation is an essential feature. As a result the implementation of a programming language consists of two stages. The implementation of the compiler and the implementation of the abstract machine[fn:17]. These two consecutive stages are the path we followed to build this project of NXP Architecture with a functional perspective. Both the abstract machine implementation and the original compiler implementation are in Emacs-Lisp.

#+CINDEX: funx
In writing the compiler, care was exercised in order to only peruse the subset of Emacs-Lisp that maps directly into the primitives of the functional language, a simple variant of LISP called ~funx~. By doing so, the compiler may compile itself to the ~funx~ abstract machine, so that further developments no longer require the original compiler source.

Additionally, the abstract machine can be ported to other programming languages with minimal effort--C, Python or Javascript being interesting targets for such ports.

Along the development of computer science abstract machines were designed and developed for imperative programming languages (Algol Object Code, UCSD P-Machine) later evolving towards abstract machines for object-oriented programming languages (Smalltalk-80, Self, Java VM). Abstract machines also abound in string processing languages[fn:18].

#+CINDEX: SECD
#+CINDEX: FAM
#+CINDEX: evaluation (strict, eager)
#+CINDEX: call-by-value
#+CINDEX: call-by-need
Abstract machines were also created for functional programming languages. The first abstract machines for functional languages, such as the SECD, @@texinfo:@xref{Landin1964,,28},@@ and FAM, defined strict evaluation, also known as eager or call-by-value evaluation, in which function arguments are evaluated before the call, and exactly once. More recently, most work has focused on lazy (or call-by-need) evaluation, in which function arguments are evaluated only if needed, and at most once.

Central concepts in abstract machines for functional languages include:
#+CINDEX: stack
- A stack :: represents the context of a nested computation. It may hold the intermediate results of pending computations, activation records of active function invocations, active exception handlers, etc.  The stack is sometimes used also for storing arguments to be passed to functions.
#+CINDEX: environment
- An environment :: maps program variables to their values.
#+CINDEX: control list
#+CINDEX: continuation
- A control list :: a sequence of controls to be executed later, representing the rest of the calculation, also called a /continuation/. Each control is simply an operation that transforms the state of the abstract machine into a new state (in perfect Turing style).
#+CINDEX: closure
#+CINDEX: thunk
- A closure :: is used to represent a function as a value.  It typically consists of a code address (for the function body) and an environment (binding the free variables of the function body)--more on closures later.
#+CINDEX: heap
#+CINDEX: dump
- A heap (dump) :: stores the data of the computation. Abstract machines usually abstract away from the details of memory management, and thus include instructions for allocating data structures in the heap, but not for freeing them; the heap is assumed to be unlimited.
#+CINDEX: garbage collector (GC)
- A garbage collector :: supports the illusion that the heap is unlimited; it occasionally reclaims unreachable heap space and makes it available for allocation of new objects.

#+CINDEX: SECD
#+CINDEX: evaluation (strict, eager)
#+CINDEX: call-by-value
The SECD machine (1964) was designed by Landin for call-by-value evaluation of the pure lambda calculus, @@texinfo:@xref{Landin1964,,28}.@@ Although there are many evolved abstract machines later developed for increasingly richer and more expressive higher-function programming languages[fn:19], we chose to focus the initial design of the NXP Architecture Abstract Machine on this seminal model. The machine derives its name from the components of its state: an evaluation stack /S/, an environment /E/, a control /C/ holding the instructions to execute, and a dump /D/ holding a continuation (i.e., a description of what must be done next) and other data.

#+CINDEX Lispkit
#+CINDEX: evaluation (lazy)
Lispkit Lisp is a lexically scoped, purely functional subset of Lisp ("Pure Lisp") developed as a testbed for functional programming concepts, @@texinfo:@xref{Bundy1984,,20}.@@ It was first used for early experimentation with /lazy evaluation/. An SECD machine-based implementation (written in an ALGOL variant) was published by the developer Peter Henderson in 1976, @@texinfo:@xref{Henderson1976,,21}.@@

Henderson later published a reference book on functional programming languages and the SECD machine, @@texinfo:@xref{Henderson1980a,,22},@@ from which the design of this experimental NXP Architecture abstract machine heavily borrows. (Misunderstandings and bugs are, however, all mine!)

*** Strict v. Lazy Evaluation
#+CINDEX: evaluation (strict, eager)
#+CINDEX: evaluation (lazy)
#+CINDEX: call-by-value
#+CINDEX: call-by-need
Two fundamental questions to the functional programming language implementor are:
  - How are function values, data values and unevaluated expressions represented?
  - How is function application performed?

#+CINDEX: sexp
Large chapters of Henderson's book detail answers to these questions in the context of the SECD model. As appropriate for a functional programming language, every expression in ~funx~ is represented as a /symbolic expression/, or /sexp/.

#+CINDEX: interpreter
Note that an /intepreter/ for a functional language like ~funx~ can be defined as a function itself ~eval :: apply( <F-SEXP>, <ARGS> )~ which basically expects a well-formed sexp, with a function value, and a list of its arguments. During interpretation, each well-formed sub-expression is identified and evaluated (often more than once). So the interpreter turns out to be a case analysis of the well-formed sub-expressions of a program.

#+CINDEX: environment
#+CINDEX: binding
Applying a function to its arguments is the /core binding operation/ in ~funx~ and other functional programming languages. Every expression in ~funx~ is evaluated with respect to an environment. An environment is just that, this association between arguments and values (which are simply sexps). In the situation where the functinal expression refers to variables introduced, not as arguments to the application, but locally--one also talks about variables defined outside the scope of the function--the environment needs to be augmented with the bindings for these variables prior to evaluation. This is the role of the ~let~ primitive in ~funx~ which introduces blocks with additional bindings.

#+NAME: foo
#+BEGIN_SRC emacs-lisp :results silent :exports code
(let
    ((foo (lambda (x) (add y (mul (quote 2) x))))
     (y (quote 4))
     )
  (foo (quote 3))
  )
#+END_SRC

In the example above, this block mechanism is used to introduce and name a new function in ~funx~. The name ~foo~ is bound to a function value (a functional sexp). The function value is called a /closure/, a composite sexp which contains both the sexp for the function--here the lambda expression--and the environment defining the values of the variables (in this instance, ~y~) and arguments (~x~) to the function. Finally the application sexp ~(foo (quote 3))~ is evaluated in an environment listing the binding of ~foo~ to this closure and the binding of ~x~ to the constant ~3~. The evaluation produces the expected value ~10~.

#+FINDEX: let
#+CINDEX: block
Note that in ~funx~ there is a single ~let~ primitive which plays the roles of both ~let~ and ~letrec~ in Lispkit. The block introduced by ~let~ may contain mutually recursive definitions.

In Cardelli's FAM, closure are represented as the text of the function and the value of its free variables. The text of a function is in itself a rather complex structure; it contains a sequence of instructions in some suitable machine language, and a set of literals which may be strings or other text cells. From a different perspective, in Henderson's SECD-based Lispkit, a function value is a suspended computation (promise to perform the computation when the value is applied to some arguments). The most compact way to represent a closure is as a block of static code (shared by all dynamic instances of the value), together with the values of its free variables.

In terms of implementation, the representation of closure could be:
  * A block of heap-allocated storage with one pointer to code followed by pointers to variables. The environment pointer points to the closure and variables are accessed by calls relative to this pointer. (Directly or by chains of pointers, with an impact on GC.)  
  * The Three Instruction Machine (TIM) takes another interesting position[fn:20]. Instead of representing a closure by a single pointer, it represents a closure by a pair of a code pointer and a pointer to a heap-allocated frame. The frame, which is a vector of code-pointer/frame-pointer pairs, gives the values of the free variables of the closure, and may be shared between many closures. 

#+CINDEX: environment (representation)
#+CINDEX: closure (representation)
In this implementation in Emacs-Lisp, environments are simply represented as association lists @@texinfo:@xref{Association Lists,,,elisp, Emacs-Lisp Info}.@@ Closure are lists constituted of the list of arguments to the function, the control list (i.e. compiled code, since we are targeting a compiler) for the functional sexp, and the environment.

#+CINDEX: evaluation (strict, eager)
#+CINDEX: evaluation (lazy)
#+CINDEX: call-by-value
#+CINDEX: call-by-need
This closure infrastructure in the SECD machine is also critical in implementing /eager/ evaluation or /lazy/ evaluation we alluded to above. In eager, or strict, evaluation, sexps are evaluated immediately as they are mentinoned (call-by-value). In a non-strict language evaluation, values are passed to functions or stored in data structures in unevaluated form, and only evaluated when their value is actually required (call-by-need). There is a long and rich literature on strict v. non strict functional languages, eager v. lazy (sometimes even /lenient/) interpreters and compilers: @@texinfo:@xref{Steele1976,,16},@@ @@texinfo:@xref{Moses1970,,17},@@ @@texinfo:@xref{Traub1991,,24},@@ @@texinfo:@xref{FriedmanWise1976,,25}.@@

#+CINDEX: thunk
#+CINDEX: promise
#+CINDEX: delay
#+CINDEX: force
Like function values, these unevaluated forms capture a /delayed/ computation, and can be represented by a closure in the same way as a function value. Following a broadly used terminology, we call this particular sort of closure a /thunk/, or a /promise/, a term which goes back to the early Algol implementations of call-by-name. When the value of the promise is required, the promise is /forced/, through naive reduction, cell model (flag) or self-updating model implementation, so that if the value is later required the value of this original evaluation is returned instead of being reevaluated. 

#+CINDEX: eval-apply model
So using the same closure concept as above, sexp evaluations can be delayed (lazy) or forced without changing the sexp itself. In eager evaluation, sexps are never delayed (or always forced). Compilers from the Lisp tradition usually compile function application as follows: evaluate the function, evaluate the argument, and apply the function value to the argument. When a known function is being applied (as is often the case, especially in Lisp), the "evaluate the
function" part becomes trivial. This model for function application, which we called earlier the /eval-apply model/, is invariably used by compilers for strict languages (eg Lisp, Hope, SML and the SECD machine in Landin's and in Henderson's papers). It is also used in some implementations of non-strict languages, except that of course only the function is evaluated before the application (eg the ABC machine[fn:21], and the <n,G>-machine[fn:22]).

#+CINDEX: push-enter model
In contrast, compilers based on lazy graph reduction treat function application as follows: push the argument on an evaluation stack, and tail-call (or enter) the function. There is no "return" when the evaluation of the function is complete. We call this the /push-enter/ model.

#+CINDEX: evaluation (lazy)
#+CINDEX: Henderson's Transformations
Henderson gives a precise definition of lazy evaluation, based on the ~delay~ and ~force~ operations, in a purely functional program (usually without any delays or forces). The rules are written as a series of program transformations:
  - delay all arguments to (user-defined) functions
  - delay all arguments to ~cons~
  - delay all definitions in ~let~ blocks
  - repeatedly force all arguments to primitive functions other than ~cons~
  - repeatedly force the test sexp in conditional expressions
  - repeatedly force the function in a function application.

@@texinfo:@xref{Henderson1980a,,22}.@@

#+CINDEX: evaluation (lazy)
#+CINDEX: parallelism
The power of lazy evaluation also allows the processing of potentially infinite structures and the proper interpretation of mutually recursive equations defined in ~let~ blocks. It also apply to the proper execution of a functional program viewed as a network of communicating processes, @@texinfo:@xref{Keller1979,,26}.@@ This turns functional languages into particularly natural forms of expressions for certain types of parallelism (and in particular, /data driven/, which is particularly useful in [[Production Systems and Rule-based Systems][rule systems]].

The next section of this manual will detail a SECD abstract machine, derived from Lispkit, with a strict compiler for the ~funx~ functional programming language. Henderson's transformations we be used however to design the ~funxp~ NXP-related extensions to the language as they require lazy compilation.

*** I/O and GUI
Two chapters in Henderson's book explain how input-output for the Lispkit LISP variant is implemented on typical hardware. Depending on the underlying implementation hardware, mapping of the abstract machine may vary and primitives for the input and output of sexps are correspondingly required.

#+CINDEX: Emacs-Lisp (implementation language)
The choice of Emacs-Lisp as an implementation programming language (and execution model) frees us from these developments for the moment. Sexps are naturally parsed and printed out by the Emacs-Lisp machinery, and the abstract ~funx~ machine is mapped almost directly to the underlying Emacs interpreter. A follow on to this project would be to port the abstract machine to another environment (such as Javascript wit Node.js), involving a new mapping of the abstract machine to the concrete one.

Consequently well-formed sexps for the ~funx~ language will be expressed here in (a subset of) Emacs-Lisp symbolic expressions.

#+CINDEX: GUI (text-based)
Similarly, Emacs being the overall environment for this abstract machine its graphical user interfaces, and user experience, would primarily be text-based. The result ~funxp~ is both a programming language and an interactive programming environment,  @@texinfo:@xref{IPE1984,,18},@@ for expert systems. It embeds NCLOSE, an earlier inference engine, in a simple--even simplistic--functional programming language called ~funx~ (/functional expressions/).

* A simplistic functional language, funx
#+CINDEX: funx
#+CINDEX: funxp
This section is devoted to the exploration of the semantics of a simplified functional programming language, called ~funx~, in which we later plan to embed a rule system, resulting in ~funxp~, an implementation of the NXP Architecture with a functional perspective.

Moreover, we are interested in developing a compiler for ~funx~ and ~funxp~. Hence after presenting the language itself, we describe the SECD-based abstract machine for the language and its implementation in Emacs-Lisp.

** FUNX a functional language, pure and simplistic
#+CINDEX: funx
Concrete ~funx~ programs are (Emacs-Lisp) sexps as per our design choices of implementation language and environment. The following sexps, where ~s~ is any sexp,  ~x~ are atoms and ~e~ well-formed expressions,  are /all/ the well-formed expressions in ~funx~:

#+CINDEX: funx (well-formed expressions)
#+CINDEX: funx (concrete syntex)
#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
#+CINDEX: variable
- ~x~ :: a variable.
#+CINDEX: constant
#+CINDEX: quote
- ~(quote s)~ :: a constant sexp.
#+CINDEX: arithmetic (add, sub, mul, div, rem)
- ~(add e1 e2)~, ~(sub e1 e2)~, ~(mul e1 e2)~, ~(div e1 e2)~, ~(rem e1 e2)~ :: arithmetic expressions (whose arguments are usually quoted when constant--a difference with Emacs-Lisp).
#+CINDEX: comparison (eq, leq)
#+CINDEX: eq (concrete syntax)
#+CINDEX: leq (concrete syntax)
- ~(eq e1 e2)~, ~(leq e1 e2)~ :: equality and comparison tests, returns a ~funx~ boolean, special atoms ~*T*~ and ~*F*~.
#+CINDEX: cell (atom, car, cdr, cons)
#+CINDEX: car (concrete syntax)
#+CINDEX: cdr (concrete syntax)
#+CINDEX: cons (concrete syntax)
- ~(car e)~, ~(cdr e)~, ~(cons e1 e2)~ :: cell structure operations.
#+CINDEX: atom (concrete syntax)
- ~(atom e)~ :: tests for atomicity, returns a ~funx~ boolean, special atoms ~*T*~ and ~*F*~.
#+CINDEX: conditional
#+CINDEX: if (concrete syntax)
- ~(if e1 e2 e3)~ :: conditional form, where ~e1~ should evaluate to a ~funx~ boolean, special atoms ~*T*~ and ~*F*~.
#+CINDEX: function (definition)
#+CINDEX: lambda (concrete syntax)
- ~(lambda (x1 ... xN) e)~ :: function definition, returns a function value.
#+CINDEX: function (application)
- ~(e e1 e2 ... eN)~ :: function application, applies function ~e~ to arguments' values ~eI~.
#+CINDEX: let (concrete syntax)
#+CINDEX: block (recursive)
- ~(let ((x1 e1) ... (xN eN)) e)~ :: recursive block, evaluates sexp ~e~ in the environment bindings ~(x1 e1) ... (xN eN)~.

These are the ~funx~ built-in functions supported by the abstract machine described in the next section. We will also see how this limited set of functions is nonetheless powerful enough to express its own compilers, eager and lazy. (It is also sufficient to express its own interpreter, should we want to skip the abstract machine step altogether.)

As an example of a program expressed in ~funx~, we state the perennial factorial example and show below a possible sexp to compute ~3!~.

#+CINDEX: factorial (example)
#+NAME: fact
#+CAPTION: The perennial factorial example, example ~ex1.lsp~.
#+BEGIN_SRC emacs-lisp :results silent :exports code
(let
    ((fac
      (lambda (n)
	(if (eq n (quote 1)) (quote 1) (mul n (fac (sub (quote 1) n))))))
     )
  (fac (quote 3))
  )
#+END_SRC

Variables are bound in ~lambda~ and ~let~ expressions. When their definitions are called their associated argument (in lambda-sexps) or local block definition (in let-sexps) are eagerly evaluated and new bindings are pushed into the environment. On exit of the function call or of the block, the variables, and their bindings in the environment are deleted. (Note that ~let~ in ~funxp~ is called ~letrec~ in Henderson's book.)

#+CINDEX: environment (as stack)
Note that using the environment both as an association list and as a stack simplifies the management of variables with identical names. The value retrieved is always the one bound in the most local binding, which is the first found when searching the environment.

Constants are always quoted--a point of departure from the Emacs-Lisp convention of not quoting numbers. Operators are defined as prefix as is common in LISP so that ~2 + 2~ is expressed as ~(add (quote 2) (quote 2))~.

Denotational semantics for Lispkit-Lisp and ~funx~ into the set category can be readily developped, leading to provably correct interpreters[fn:23]. The classic difficulty is the sementics of the ~let~ recursive block operator. (The non-recursive ~let~, as in Henderson's Lispkit, is simply reduced to an equivalent ~lambda~ expression, ~(let (x e) e-body)~ being ~(lambda (x) e-body)~ applied to argument ~e~, and presents no difficulty.) The specificity of recursive block is that each variable is evaluated in an environment that binds all the variables in the ~let~ block, so that this /augmented/ environment is a form of fixed point in the operation of binding variables.

** FUNX SECD Abstract Machine
#+CINDEX: SECD
In this section we borrow freely from Henderson's book for the description of the SECD machine, @@texinfo:@xref{Henderson1980a,,22}.@@ The SECD machine, invented in its original form by Landin, derives its name from the designation of its four principal components, or registers.

- stack :: used to hold values of expressions during computation.
- environment :: used to hold bindngs of variables to values during computation.
- control list :: used to hold the abstract machine program being executed.
- dump :: used as a general stack to hold values of other registers, particularly when calling functions

#+CINDEX: state (of SECD machine)
#+CINDEX: control list
#+CINDEX: continuation
All registers hold sexps that usually are lists of sexps. The entire state of the SECD machine is given by giving the content of its four registers. The program of a SECD machine is also a sexp, a list of /control-sexps/ usually atomic, which are read sequentially. Each individual control transforms the state of the SECD machine, so that each is an instruction for a machine transition, written as:

~s e c d -> s' e' c' d'~

The control list constitutes the program of the abstract machine and tells it how to move from state to state, following the sequence of machine transistions each individual control in the list denotes[fn:25]. (Control lists are also called continuations in this info file.)

For instance, and with a bit of heads-on over the next section, let us look at the control list for simple arithmetics:

#+BEGIN_SRC emacs-lisp :results silent :exports code
(mul (quote 2) (quote 3))
#+END_SRC

The expected control list for the above ~funx~ sexp would be:

#+BEGIN_SRC emacs-lisp :results silent :exports code
(LDC 2 LDC 3 MUL STOP)
#+END_SRC

It introduces three controls:

- LDC :: Pushes the following constant sexp on the stack.
- MUL :: Pops values from the stack twice, multiplies them and pushes back the result on the stack.
- STOP :: Stops the abstract machine, leaving the state untouched.

And the natural execution of this program with initally nulled stack, environment and dump registers would look like the following sequence of transitions:

| Sequence | s     | e   | c                      | d   |
|----------+-------+-----+------------------------+-----|
|        0 | nil   | nil | (LDC 2 LDC 3 MUL STOP) | nil |
|        1 | (2)   | nil | (LDC 3 MUL STOP)       | nil |
|        2 | (3 2) | nil | (MUL STOP)             | nil |
|        3 | (6)   | nil | (STOP)                 | nil |

with the result nicely sitting on top of the stack when the machine finally stops.

*** Operational semantics
#+CINDEX: semantics (operational)
#+CINDEX: mnemonics (control)
The operational semantics of the abstract machine is then defined by each control and its associated machine transition. The mnemonics for the controls in the abstract machine are as follows:


#+CINDEX: LD
- LD :: push variable's value on the stack
#+CINDEX: LDC
- LDC :: push constant on the stack
#+CINDEX: LDF
- LDF :: push function-plus-environment closure on the stack

#+CINDEX: ADD
- ADD :: apply arithmetic operator to top two stack values, push result back
#+CINDEX: SUB
- SUB :: apply arithmetic operator to top two stack values, push result back
#+CINDEX: MUL
- MUL :: apply arithmetic operator to top two stack values, push result back
#+CINDEX: DIV
- DIV :: apply arithmetic operator to top two stack values, push result back
#+CINDEX: REM
- REM :: apply arithmetic operator to top two stack values, push result back

#+CINDEX: CAR
- CAR :: replace top of stack by its head
#+CINDEX: CDR
- CDR :: replace top of stack by its tail
#+CINDEX: CONS
- CONS :: pop top two stack values, push back /cons/ of values
#+CINDEX: ATOM
- ATOM :: test for atomicity, replace top of stack by boolean ~*T*~, ~*F*~
#+CINDEX: EQ
- EQ :: test for equality of popped top two stack values, push boolean result
#+CINDEX: LEQ
- LEQ :: test for lower-than popped top two stack values, push boolean result
#+CINDEX: SEL
- SEL :: select control for ~if~ sexps, followed by two arguments in the control list: sexp if top of stack is ~*T*~, and sexp if top of stack is ~*F*~
#+CINDEX: JOIN
- JOIN :: terminates a sexp in SEL control, and proceeds to the next control after SEL in list

#+CINDEX: AP
- AP :: apply function on top of stack, push result back
#+CINDEX: RTN
- RTN :: terminates a function definition in LDF argument, push the top of the stack as the result of the function application
#+CINDEX: DUM
- DUM :: opens a recursive ~let~ block
#+CINDEX: RAP
- RAP :: recursive apply, evaluate sexp in a recursive ~let~ block

In order to facilitate the presentation we grouped the controls in several categories:

**** ld-group
The load group of controls usually push a value on the stack.

#+FINDEX: LDC (transition)
#+FINDEX: LD (transition)
#+FINDEX: LDF (transition)
| s                   | e | c                | d |
|---------------------+---+------------------+---|
|                     |   |                  |   |
| s                   | e | (LDC sexp . c)   | d |
| (sexp . s)          | e | c                | d |
|                     |   |                  |   |
|---------------------+---+------------------+---|
|                     |   |                  |   |
| s                   | e | (LD x . c )      | d |
| ((~locate~ x e) . s | e | c                | d |
|                     |   |                  |   |
|---------------------+---+------------------+---|
|                     |   |                  |   |
| s                   | e | (LDF f-sexp . c) | d |
| ((f-sexp e) . s)    | e | c                | d |
|                     |   |                  |   |
|---------------------+---+------------------+---|

#+FINDEX: locate (variable in environment)
#+FINDEX: LDF (closure)
#+CINDEX: closure (LDF)
In the transition table above, ~locate~ is a environment utility function which given a variable name and an environment returns the value bound to the variable in the environment, or ~nil~ if no such binding is found. Note that ~sexp~ denotes any sexp, and ~f-sexp~ denotes a function sexp (i.e. a list which head is a list of named arguments, and tail is the control list for the body of the function), so that the ~LDF~ control pushes a closure (function + environment) on the stack.

**** arith-group
#+CINDEX: arithmetic
The machine transitions for the arithmetic group all have the same form:

#+FINDEX: ADD (transition)
#+FINDEX: SUB (transition)
#+FINDEX: MUL (transition)
#+FINDEX: DIV (transition)
#+FINDEX: REM (transition)
| s              | e | c          | d |
|----------------+---+------------+---|
|                |   |            |   |
| (a b . s)      | e | (<OP> . c) | d |
| (<a OP b> . s) | e | c          | d |
|                |   |            |   |
|----------------+---+------------+---|

**** ops-group
#+FINDEX: CAR (transition)
#+FINDEX: CDR (transition)
#+FINDEX: CONS (transition)
#+FINDEX: ATOM (transition)
#+FINDEX: LEQ (transition)
#+FINDEX: EQ (transition)
#+FINDEX: SEL (transition)
#+FINDEX: JOIN (transition)
This group contains the basic operations on values in ~funx~.

| s              | e | c               | d     |
|----------------+---+-----------------+-------|
| ;; Lists       |   |                 |       |
|                |   |                 |       |
| ((a . b) s)    | e | (CAR . c)       | d     |
| (a . s)        | e | c               | d     |
|                |   |                 |       |
| ((a . b) s)    | e | (CDR . c)       | d     |
| (b . s)        | e | c               | d     |
|                |   |                 |       |
| (a b s)        | e | (CONS . c)      | d     |
| ((a . b) s)    | e | c               | d     |
|                |   |                 |       |
|----------------+---+-----------------+-------|
| ;; Tests       |   |                 |       |
|                |   |                 |       |
| (a s)          | e | (ATOM . c)      | d     |
| ([*T*,*F*] s)  | e | c               | d     |
|                |   |                 |       |
| (a b s)        | e | (EQ . c)        | d     |
| ([*T*,*F*] s)  | e | c               | d     |
|                |   |                 |       |
| (a b s)        | e | (LEQ . c)       | d     |
| ([*T*,*F*] s)  | e | c               | d     |
|                |   |                 |       |
|----------------+---+-----------------+-------|
| ;; IF control  |   |                 |       |
|                |   |                 |       |
| ([*T*,*F*]  s) | e | (SEL cT cF . c) | d     |
| s              | e | [cT, cF]        | (c d) |
|                |   |                 |       |
| s              | e | (JOIN)          | (c d) |
| s              | e | c               | d     |
|                |   |                 |       |
|----------------+---+-----------------+-------|

Notes: ~funx~ provides only simple primitive tests: atomicity, equality (implemented here as LISP's ~EQUAL~) and comparison of numbers. The conditional control is implemented by pushing the continuation on the dump register and entering the appropriate control list branch; the ~JOIN~ control always terminates each branch, and reinstalls the continuation from the dump register. (There are other ways to implement the conditional control, of course.)

**** fun-group
This group implements the function application and lambda-function definitions.

#+FINDEX: AP (transition)
#+FINDEX: RTN (transition)
#+FINDEX: DUM (transition)
#+FINDEX: RAP (transition)
| s               | e                               | c         | d         |
|-----------------+---------------------------------+-----------+-----------|
| ;; Application  |                                 |           |           |
|                 |                                 |           |           |
| (f-sexp vals s) | e                               | (AP c)    | d         |
| nil             | ((args-f-sexp . vals) e-f-sexp) | cl-f-sexp | (s e c d) |
|                 |                                 |           |           |
| (ret-val)       | e'                              | (RTN)     | (s e c d) |
| (ret-val s)     | e                               | c         | d         |
|                 |                                 |           |           |
|-----------------+---------------------------------+-----------+-----------|
| ;; ~let~ block  |                                 |           |           |
|                 |                                 |           |           |
| s               | e                               | (DUM c)   | d         |
| s               | (OMEGA e)                       | c         | d         |
|                 |                                 |           |           |
| (f-sexp vals s) | (OMEGA e)                       | (RAP c)   | d         |
| nil             | ~rplaca~                        | cl-f-sexp | (s e c d) |
|                 | ((args-f-sexp . vals) e-f-sexp) |           |           |

#+CINDEX: push-enter
Notes: the ~AP~ control works in association with the ~LDF~ which loads a closure on the stack. It pushes the state to the dump register and enters the control list in the closure, within an environment resulting from pushing the association list of function arguments and values from the stack to the environment stored in the closure. The ~RTN~ control, which always terminates a control list in a closure, reinstates the machine state from the dump register and pushes the result (top of the stack) of the function application to the recovered stack.

- args-f-sexp :: extracts the argument names from the closure sexp
- vals :: popped values from the stack, on for each function argument in order
- e-f-sexp :: extracts the environment from the closure sexp
- cl-f-sexp :: extracts the control list from the closure sexp
- OMEGA :: a litteral constant used as placeholder for the ~car~ of the environment which is altered by,
- rplaca :: replaces OMEGA in its argument by a pointer the association list of function arguments and values from the stack, and returns a pointer to itself (this implements the recursive nature of the ~let~ block)

The ~DUM~ control opens a ~let~ recursive block which expects a ~LDF <closure> RAP~ to terminate it, where the environment in the closure will be altered in-place by pushing the variable definitions in the ~let~ statement.

As an example, the factorial function in ~funx~ from above:

#+CINDEX: factorial (example)
#+CAPTION: The perennial factorial example, example ~ex1.lsp~.
#+BEGIN_SRC emacs-lisp :results silent :exports code
(let
    ((fac
      (lambda (n)
	(if (eq n (quote 1)) (quote 1) (mul n (fac (sub (quote 1) n))))))
     )
  (fac (quote 3))
  )
#+END_SRC


#+CINDEX: factorial (compiled)
#+NAME: fact-secd
#+CAPTION: The perennial factorial example compiled, 3! in ~ex1.fasl~.
#+BEGIN_SRC emacs-lisp :results silent :exports code
(DUM 
 LDF ((n) LDC 1 
          LD n 
          EQ 
          SEL (LDC 1 JOIN) 
              (LD n LDC 1 SUB LD fac AP LD n MUL JOIN) 
          RTN) 
 LDF ((fac) LDC 3 LD fac AP RTN) 
 RAP 
 STOP)
#+END_SRC

*** Running the funx SECD Abstract Machine
#+CINDEX: SECD Machine (running)
The executive for the SECD Abstract Machine is found in the ~secd-exec.el~ file. It is a simple automata that moves the state of the machine to the next one according to the values of its registers ~s, e, c,~ and ~d~. At each step the transition is looked up into the representation of last section's transition tables, and the transformation of state indicated is performed until the special control ~STOP~ is found.

#+ATTR_TEXINFO: :options secd-cycle stack environment control dump
#+BEGIN_defun
  Steps through the control list /control/, with initial stack, environment and dump as provided and stops at ~STOP~ control, returning the state of the stopped machine as the list ~(s e c d)~.
#+END_defun

#+BEGIN_SRC emacs-lisp :results silent :exports code
(secd-cycle nil nil '(LDC 2 LDC 3 MUL STOP) nil)
    => ((6) nil (STOP) nil)
#+END_SRC

*** A funx strict compiler
#+CINDEX: funx (strict compiler)
#+CINDEX: call-by-value
The strict compiler translates ~funx~ sexps to SECD Abstract Machine control lists. It is strict, or eager, in the sense that in function applications, arguments are evaluated first, in left-to-right-order, and pushed on the stack; the function code is pushed on the stack and the application control, ~AP~ or ~RAP~ is executed. This is also known as /call-by-value/.

#+CINDEX: call-by-name
Of course, other strategies are possible. For instance, rather than evaluating the arguments first, which might not be used in the body of the function thus wasting precious computing resources, the translation could substitute the evaluation code for the arguments in the body of the function. This is the /call-by-name/ convention rule. (Note that using call-by-name, functions may be defined even for arguments which computation would not terminate in call-by-value evaluation.)

Following Henderson's book, the strict compiler implements the following translation of sexps:

| ~funx~ sexp                     | SECD Control List                               |
|---------------------------------+-------------------------------------------------|
| T v                             | LD v                                            |
| T (quote s)                     | LDC s                                           |
| T (add s1 s2)                   | T s2 T s1 ADD                                   |
| T (leq s1 s2)                   | T s2 T s1 LEQ                                   |
| T (atom s)                      | T s ATOM                                        |
| T (car s)                       | T s CAR                                         |
| T (cdr s)                       | T s CDR                                         |
| T (cons s1 s2)                  | T s2 T s1 CONS                                  |
| T (if s1 s2 s3)                 | T s1 SEL (T s2 JOIN) (T s3 JOIN)                |
| T (lambda (v1 ... vn) s)        | LDF ((v1 ... vn) T s RTN)                       |
| T (let ((v1 s1) ... (vn sn)) s) | DUM T sn ... T s1 LDF ((v1 ... vn) T s RTN) RAP |
| T (s s1 ... sn)                 | T sn ... T s1 T s AP                            |


where ~v~, ~s1 ... sn~ are variable names, the value of which will be found in the environment register of the SECD machine; and ~s~, ~s1 ... sn~ are well-formed ~funxp~ sexps. (The other arithmetic sexps are translated like ~add~.)

Note that there are minor differences with the translation in Henderson's book, related to the implementation of the environment and to the single ~let~ sexp, which does note distinguishes between recursive and non-recursive blocks.

In Emacs-Lisp the strict compiler is invoked by:

#+ATTR_TEXINFO: :options secd-comp--comp expression names controls
#+BEGIN_defun
    /expression/ is the ~funx~ sexp to translate; /names/ is a register of variable names defined in /expression/: this is unused in the strict compiler, but will be used in the NXP lazy variant of the compiler; /controls/ is the recursively built list of controls.
#+END_defun

#+BEGIN_SRC emacs-lisp :results silent :exports code
(secd-comp--comp '(mul (quote 2) (quote 3)) nil '(STOP))
    => (LD 3 LD 2 MUL STOP)
#+END_SRC

**** Another perennial example, compiling the compiler
As a footnote to this section, we could not skip the "compile the compiler" exercize! The simple ~funx~ functional language is expressive enough to implement the translation to SECD Abstract Machine control lists. The previous example is found in the ~examples/comp3.lsp~ file, which once compiled with the Emacs-Lisp ~secd-comp--comp~ function above produces the following control list, with the ~funxp~ sexp ~'(mul (quote 2) (quote 3))~ to be compiled loaded (as a constant, ~LDC~) on the stack.

#+BEGIN_SRC emacs-lisp :results silent :exports code
  (DUM LDF ((elist) LD nil LD elist EQ SEL (LD nil JOIN) (LD elist
  CDR LD secd-comp--vars AP LD elist CAR CAR CONS JOIN) RTN)
  LDF ((elist n c) LD nil LD elist EQ SEL (LD c JOIN) (LD c LD n LD
  elist CAR CDR CAR LD secd-comp--comp AP LD n LD elist CDR LD
  secd-comp--list AP JOIN) RTN) LDF ((e n c) LD e ATOM SEL (LD c LD
  e CONS LDC LD CONS JOIN) (LDC car LD e CAR EQ SEL (LD c LDC CAR
  CONS LD n LD e CDR CAR LD secd-comp--comp AP JOIN) (LDC cdr LD e
  CAR EQ SEL (LD c LDC CDR CONS LD n LD e CDR CAR LD
  secd-comp--comp AP JOIN) (LDC atom LD e CAR EQ SEL (LD c LDC ATOM
  CONS LD n LD e CDR CAR LD secd-comp--comp AP JOIN) (LDC quote LD
  e CAR EQ SEL (LD c LD e CDR CAR CONS LDC LDC CONS JOIN) (LDC cons
  LD e CAR EQ SEL (LD c LDC CONS CONS LD n LD e CDR CAR LD
  secd-comp--comp AP LD n LD e CDR CDR CAR LD secd-comp--comp AP
  JOIN) (LDC eq LD e CAR EQ SEL (LD c LDC EQ CONS LD n LD e CDR CAR
  LD secd-comp--comp AP LD n LD e CDR CDR CAR LD secd-comp--comp AP
  JOIN) (LDC leq LD e CAR EQ SEL (LD c LDC LEQ CONS LD n LD e CDR
  CAR LD secd-comp--comp AP LD n LD e CDR CDR CAR LD
  secd-comp--comp AP JOIN) (LDC add LD e CAR EQ SEL (LD c LDC ADD
  CONS LD n LD e CDR CAR LD secd-comp--comp AP LD n LD e CDR CDR
  CAR LD secd-comp--comp AP JOIN) (LDC sub LD e CAR EQ SEL (LD c
  LDC SUB CONS LD n LD e CDR CAR LD secd-comp--comp AP LD n LD e
  CDR CDR CAR LD secd-comp--comp AP JOIN) (LDC mul LD e CAR EQ
  SEL (LD c LDC MUL CONS LD n LD e CDR CAR LD secd-comp--comp AP LD
  n LD e CDR CDR CAR LD secd-comp--comp AP JOIN) (LDC div LD e CAR
  EQ SEL (LD c LDC DIV CONS LD n LD e CDR CAR LD secd-comp--comp AP
  LD n LD e CDR CDR CAR LD secd-comp--comp AP JOIN) (LDC rem LD e
  CAR EQ SEL (LD c LDC REM CONS LD n LD e CDR CAR LD
  secd-comp--comp AP LD n LD e CDR CDR CAR LD secd-comp--comp AP
  JOIN) (LDC if LD e CAR EQ SEL (LDC (JOIN) LD n LD e CDR CDR CDR
  CAR LD secd-comp--comp AP LDC (JOIN) LD n LD e CDR CDR CAR LD
  secd-comp--comp AP LDF ((cont-t cont-f) LD c LD cont-f CONS LD
  cont-t CONS LDC SEL CONS LD n LD e CDR CAR LD secd-comp--comp AP
  RTN) AP JOIN) (LDC lambda LD e CAR EQ SEL (LD c LDC (RTN) LD n LD
  e CDR CDR CAR LD secd-comp--comp AP LD e CDR CAR CONS CONS LDC
  LDF CONS JOIN) (LDC let LD e CAR EQ SEL (LD c LDC RAP CONS
  LDC (RTN) LD n LD e CDR CDR CAR LD secd-comp--comp AP LD e CDR
  CAR LD secd-comp--vars AP CONS CONS LDC LDF CONS LD n LD e CDR
  CAR LD secd-comp--list AP LDC DUM CONS JOIN) (LD c LDC AP CONS LD
  n LD e CAR LD secd-comp--comp AP LD n LD e CDR LD secd-comp--args
  AP JOIN) JOIN) JOIN) JOIN) JOIN) JOIN) JOIN) JOIN) JOIN) JOIN)
  JOIN) JOIN) JOIN) JOIN) JOIN) JOIN) RTN) LDF ((secd-comp--comp
  secd-comp--list secd-comp--vars) LD nil LD nil LDC (mul '2 '3) LD
  secd-comp--comp AP RTN) RAP STOP)
#+END_SRC

Running the SECD machine on this control list then produces the expected:

#+BEGIN_SRC emacs-lisp :results silent :exports code
    => (LD 3 LD 2 MUL STOP)
#+END_SRC

which run again through the SECD machine will finally output ~6~!

* Extending for NXP-style inferencing, funxp
#+CINDEX: funxp
#+CINDEX: delay-force
In order to build the NXP Architecture on top of the ~funx~ functional programming language, we will first extend the features of the language with Henderson's /delay-force/ mechanism for delayed evaluations of sexps.

#+CINDEX: promises
#+CINDEX: delayed evaluations
While delayed evaluations expose some of the parallel nature of functional programming languages, where sexps are viewed as separate threads in a network of communicating processes, we will not explore this further in the current implementations[fn:26].

Similarly, while delayed evaluations make it possible to write functional programs which process infinite structures, e.g. infinite lists, this is a feature we will not explore in the following sections.

#+CINDEX: agenda
#+CINDEX: heuristics
The NXP Architecture uses delayed evaluations for all evaluations of facts, conditions, rules and hypotheses defined in NXP knowledge bases. The proper order of execution of these delayed evaluations, based on one or more heuristics, constitutes the control structure of an interactive session, also called the /agenda/. In the current implementation, the agenda is simply represented by the global control list of the underlying SECD Machine.

In order to emphasize delayed evaluations, we call the functional language ~funx~ augmented with delayed evaluations: ~funxp~.

** Promises are delayed evaluations
#+CINDEX: promises
#+CINDEX: delayed evaluations
#+CINDEX: lazy evaluation
Delayed evaluations lead to lazy evaluation of sexps, compared to typical call-by-value eager evaluations @@texinfo:@xref{Henderson1976,,21}.@@ It is usually introduced explicitly in augmenting the well-formed expression with explicit delaying and forcing evaluation, so that ~(delay e)~ and ~(force e)~ are well-formed sexps when ~e~ is a well-formed sexp, and ~(force (delay e))~ has the same value as ~e~.

#+CINDEX: delay
#+CINDEX: promise
The value of ~(delay e)~ is a closure-like structure, we call a /promise/. It incorporates a representation of the expression--its compiled control list in the compiler output--and the environment in which to be evaluated.

Note that one interpretation of the delay/force could readily be implemented in ~funx~ without extension. Making an expression ~e~ into a function with no parameter ~(lambda () e)~ has indeed the effect of postponing the evaluation until it is actually called with no argument, ~((lambda () e))~. This intepretation however would cause the function to be evaluated twice if called twice as opposed to once only, as in ~(let ((val (lambda () e))) (cons (val) (val)))~.

An alternative interpretation would be that the value of the delayes function is represented by a closure containing ~e~ which, when forced, is replaced by the value resulting from the evaluation of ~e~. The next forcing would return this previous evaluation result rather than trigger a new evaluation[fn:27]. The ~funx~ control set is thus expanded to make the latter intepretation precise:

| ~funxp~ feature | Control lists |
|-----------------+---------------|
| T (delay s)     | LDE T s UPD   |
| T (force s)     | T s AP0      |
| T NXP-s         | LDP NXP-s     |

where ~NXP-s~ designates an NXP special sexp (fact, condition, rule, action or hypothesis), and introducing new controls:

#+CINDEX: LDE
- LDE :: load the following promise on stack
#+CINDEX: LDP
- LDP :: load the named promise from the environment
#+CINDEX: AP0
- AP0 :: evaluate the promise on top of the stack, stores the result of the evaluation in the promise, or if already evaluated, use that value
#+CINDEX: UPD
- UPD :: terminates a promise definition (in LDE argument)

*** lazy-group
Transitions for the new controls are as follows: 

#+FINDEX: LDE (transition)
#+FINDEX: LDP (transition)
#+FINDEX: UPD (transition)
#+FINDEX: AP0 (transition)
| s                   | e  | c            | d                             |
|---------------------+----+--------------+-------------------------------|
|                     |    |              |                               |
| s                   | e  | (LDE c . c)  | d                             |
| ((PROMISE c e) . s) | e  | c            | d                             |
|                     |    |              |                               |
|---------------------+----+--------------+-------------------------------|
|                     |    |              |                               |
| s                   | e  | (LDP x . c ) | d                             |
| ((~locate~ x e) . s | e  | c            | d                             |
|                     |    |              |                               |
|---------------------+----+--------------+-------------------------------|
|                     |    |              |                               |
| ((v e) . s)         | e  | (AP0 . c)    | d                             |
| (v . s)             | e  | c            | d                             |
|                     |    |              |                               |
|---------------------+----+--------------+-------------------------------|
|                     |    |              |                               |
| ((PROMISE c e) . s) | e' | (AP0 . c')   | d                             |
| nil                 | e  | c            | (((PROMISE c e) . s) e' c' d) |
|                     |    |              |                               |
|---------------------+----+--------------+-------------------------------|
|                     |    |              |                               |
| (v)                 | e  | (UPD)        | (((PROMISE c e) . s) e' c' d) |
| (v . s)             | e' | c'           | d                             |
|                     |    |              | and (PROMISE c e) => (v e)    |
|                     |    |              |                               |
|---------------------+----+--------------+-------------------------------|

#+CINDEX: promise
Note that the promise is distinct from a standard closure, having the reserverd keyword ~PROMISE~ when delayed. The ~AP0~ control first checks for this keyword and evaluates the closure as usual if delayed, or returns the result of the previous evaluation if present. (Hence the two lines in the previous table.)

The ~UPD~ control has an additional side-effect which is to update the promise with the result of the evaluation for further reference. Here we rely on the notion than only one update is necessary to alter the promise in place. (Implementations should probably never copy promises and systematically use pointers to premises to alleviate the consistency checks.) Updates affect all the subsequent accesses to the promise.

*** Lazy evaluation
#+CINDEX: lazy evaluation
#+CINDEX: lazy compiler
#+CINDEX: non-strict compiler
With this feature added in ~funxp~ the precise definition of a lazy evaluator (or compiler for that matter) is obtained by considering a strict evaluator (or compiler) and making the following changes throughout the entire program:

  - delay all arguments to user-defined function sexps.
  - delay both arguments to ~cons~, @@texinfo:@xref{FriedmanWise1976,,25}.@@ 
  - delay all definitions in ~let~ recursive blocks.
  - repeatedly force all arguments to primitives other than ~cons~ (e.g. ~car~ ...)
  - repeatedly force the test in ~if~ sexps.
  - repeatedly force the function in applications.

These can be written as simple program /transformations/ performed just before evaluation or compilation of ~funxp~ sexps.

** Rules and Knowledge Bases
#+CINDEX: interactivity
#+CINDEX: agenda
We are now ready to introduce the NXP Architecture specific functional expressions to complete the specification of ~funxp~. All the new well-formed sexps are related to the core novel issues brought forward in rule systems: the issue of /interactivity/ with the user, and more generally with external sources of data required by the inference process on the one hand, and the issue of the control structure(s) in problem-soving--a notoriously complex question [fn:28].

This section focuses on the new constructs introduced by the NXP Architecture rule system. It also provides some motivations for their introduction and intended use in problem representation and problem-solving performance. The next section explains the ~funxp~ compiler which translate these constructs into executable control lists on the SECD Abstract Machine.

#+CINDEX: rule (sexp)
#+CINDEX: LHS (sexp)
#+CINDEX: RHS (sexp)
#+CINDEX: hypothesis (sexp)
#+CINDEX: condition (sexp)
#+CINDEX: action (sexp)
#+CINDEX: context (sexp)
The general rule sexp in ~funxp~ is as follows:

#+ATTR_TEXINFO: :options rule hypo lhs &optional rhs context
#+BEGIN_defun
  A rule sexp where /hypo/ is an hypothesis, or goal; /lhs/, for /left-hand side/ (LHS) is a list of conditions; optional /rhs/, for /right-hand side/ (RHS) is a list of actions; and optional /context/ is a list of sexp property.
#+END_defun

where:

- hypothesis :: also called a goal, or a hypo, is a symbol which conventionally evaluates to true (~*T*~) when all the conditions in the left-hand side are matched and to false (~*F*~) if any one (or more) of the conditions in the left-hand side does not match.
- lhs :: a list of conditions.
- condition :: a sexp which is a ~funxp~ expression that evaluates to true (~*T*~) or false (~*F*~) exclusively.
- rhs :: a list of actions.
- action :: a sexp exclusively of the form ~(set s sexp)~ where ~s~ is a symbol and ~sexp~ a well-formed ~funxp~ expression that calculates the value assigned to ~s~.
- context :: is a property written as ~:context sexp~ where ~sexp~ a well-formed ~funxp~ expression, usually a list of symbols.

#+CINDEX: hypothesis (true)
The motivation for this compact form is to express that a rule says that when all its condions in the LHS are true (matched by values of data in working memory), the rule /fires/, becomes true,  and the hypothesis becomes true.

#+CINDEX: knowledge base (sexp)
A knowledge base is a list of rule sexps.

#+CINDEX: hypothesis (false)
Note that within a knowledge base, a given hypothesis may appear in several rules. By convention, during evaluation if all of these rules evaluate to false (and consequently none is fired) the hypothesis becomes false.

#+CINDEX: hypothesis (neither true, nor false)
Hence a hypo turns out true if at least one of its rules in the knowledge base is true (and fired); it turns out false if all of its rules in the knowledge base are evaluated to false (hence none fired); and remains /unknown/ if only some of its rules have been evaluated, and to false.


*** Interactive user interfaces
#+CINDEX: rule system
#+CINDEX: working memory
In rule systems, see  [[Production Systems and Rule-based Systems]], rules are matched against data which resides in /working memory/. In the current implementation we opt for mapping the working memory into the environment register of the SECD Abstract Machine, and designates each datum simply by a symbol in ~funxp~.

#+CINDEX: ASK
#+CINDEX: Question (rule system)
Values for these data may be computed, evaluated, or deduced by the rule system as the problem-solving progresses. Values might also be required, or collected from data sources external to the rule system itself, including from the user. The I/O channel between the rule system and its peripheral systems involves, in its generality, issues of sync/async control in communications. The current implementation simplifies the latter by considering only one data source, the user, and consequently one extension to the set of well-formed ~funxp~ sexps, the /question/. We will see that the management of the question is quite generic and may be easily adapted to other data source, e.g. relational databases or Web Services queries.

The newly introduced well-formed sexp is: ~(question s)~ where ~s~ is a symbol, which naturally translates to "What is the value of s?" (Later, we show how to customise text prompts for the text-based user interface in Emacs.)

#+CINDEX: session
#+CINDEX: protocol
This new sexp also introduces the new convention that evaluation stops at a question, waiting for an answer from the user to resume the evaluation at some later point in time. This makes the evaluation by the rule system interactive, as several questions might be asked in the process of running the system. The full cycle of interaction with the rule system, until it terminates, is called a /session/. Sessions are reminiscent of the famous /protocols/ studied by Newell and Simon as a method for eliciting knowledge from human experts.

In the current implementation, the user being the single data source, any symbol, the required value of which is not present in working memory nor can be computed or evaluated by the rule system, is deemed collectable from external data sources, i.e. the user, and a corresponding question is output and evaluation stops.

Note that since more than one data collection for different rules could be going on simultaneously from an evaluation perspective, there could logically be several pending questions at any given output. In the current implementation, as mentioned early, we did not address issues of parallelism which could gracefully handle the synchronization of inference threads within the framework of the ~funxp~ delayed evaluations. Instead we will introduce NXP-specific control structures to captures the proper sequencing of interaction with the user--an important criterion of usability of expert systems, @@texinfo:@xref{Szolovits1982,,33}.@@

Note also that this mechanism may easily be extended to other data sources by introducing similar well-formed expressions, possibly arranged in a stack where failure to find the value of the required symbol in the top of the stack data source cause popping it out and trying the next one, addressing the question to the user at a last resort.

*** Goal-driven evaluation
*** Event-driven evaluation
*** Heuristics design space

Promises and delay/force. A mention of thread and parallelism (QLISP, Kugel non-halting computations v. thinking).

Data-driven vs. call: Hewitt mentioned in Sacerdoti as pattern-directed function invocation.

NXP-style rules. Rule: Hypo LHS &optional RHS &optional :context.

Glossary of terms: hypo(thesis), cond(itions), LHS/RHS, actions, sign, goal/subgoal, backward/forward chaining, knowcess, gating...

* Compiling funxp
#+BEGIN_QUOTE
What seemed to be missing was the desire to compile.

--- Don De Lillo, /The Players/
#+END_QUOTE

cps-group, io-group

Knowledge base, or rule sets, are compiled to funxp environments.

An environment is the state of the working memory + hidden information used by the engine (plus top level control list in d). Why not put everything in d?

Richard Fikes, Deductive Retrieval Mechanisms for State Description Models, 1975, SRI Technical Report (Note) 106. For other ideas to represent working memory. Also BRAND X semantic nets.

Decorations and globales.

* An Emacs-based client

** Session. Interactivity. Trace and protocol.

** Encyclopedia and tree representation. Commands.
#+CINDEX: Encylopedia

#+ATTR_TEXINFO: :table-type vtable 
#+BEGIN_QUOTE
  - `q' :: Kill Encyclopedia buffer.
  - `k' :: Suggest hypo at point and knowcess.
  - `w' :: Volunteer, or What-if, data at point and knowcess.
  - `a' :: Answer pending question and resume session.
  - `r' :: Restart session.
  - `t' :: Open backward-chaining tree of hypo at point.
#+END_QUOTE

* Bibliography
Source: ~funx.bib~.

#+NAME: bibliography
#+BEGIN_SRC emacs-lisp :results value raw :exports results 
  (require 'parsebib)
  (require 'subr-x)

  (defun funx-parse (fname)
    (with-temp-buffer
      (insert-file-contents fname)
      (parsebib-collect-entries)))

  (defun funx-trim (str)
    (let ((re "[ \t\n\r\"{}]+"))
      (string-trim-left (string-trim-right str re) re)))

  (defun funx-first (keys alist)
    (if (null keys) ""
      (if (assoc (car keys) alist)
	  (cdr (assoc (car keys) alist))
	(funx-first (cdr keys) alist))))

  (let ((nref 0)
	(outstr "\n\n")
	(funx-bib (funx-parse "C:/Users/jmc/Documents/code/funx/funx.bib")))
    (maphash
     #'(lambda (key value)
	 (setq nref (1+ nref))
	 (setq outstr
	       (concat
		outstr
		(format
		 "@@texinfo:@anchor{%s}@@%d. %s. /%s/. %s, %s.\n\n"
		 key nref
		 (funx-trim (cdr (assoc "author" value)))
		 (funx-trim (cdr (assoc "title"  value)))
		 (funx-trim
		  (funx-first '("publisher" "journal" "institution") value))
		 (funx-trim (cdr (assoc "year"   value))))
		)))
     funx-bib)
     outstr)
#+END_SRC
* Test WIP                                                         :noexport:

From funx.bib

@@texinfo:@anchor{Rougier2005}@@ 1. Rougier, Nicolas P. and Noelle, David C. and Braver, Todd S. and Cohen, Jonathan D. and O{\textquoteright}Reilly, Randall C., Prefrontal cortex and flexible cognitive control: Rules without symbols, (2005) National Academy of Sciences

@@texinfo:@anchor{Newell1972}@@ 2. Newell, Allen and Simon, H. A., Human Problem Solving, (1972) Prentice-Hall, Inc.

@@texinfo:@anchor{PDIS1978}@@ 3. Waterman, D. A. and Hayes-Roth, Frederick, Pattern-Directed Inference Systems, (1978) Academic Press, Inc.

@@texinfo:@anchor{Post1943}@@ 4. Emil L. Post, Formal Reductions of the General Combinatorial Decision Problem, (1943) Association for Symbolic Logic

@@texinfo:@anchor{Markov1957}@@ 5. A. A. Markov, Theory of Algorithms, (1957) Association for Symbolic Logic

@@texinfo:@anchor{Floyd1961}@@ 6. Floyd, Robert W., An Algorithm for Coding Efficient Arithmetic Operations, (1961) Association for Computing Machinery

@@texinfo:@anchor{Chomsky57}@@ 7. Chomsky, Noam, Syntactic Structures, (1957) Mouton and Co.

@@texinfo:@anchor{Paycha1963}@@ 8. Paycha, F., Cybern{\'e}tique de la consultation: logique et morale de la m{\'e}decine, (1963) Gauthier-Villars

@@texinfo:@anchor{Rappaport-1984-15190}@@ 9. Alain Rappaport and Jean-Marie C. Chauvet, Symbolic Knowledge Processing for he Acquisition of Expert Behavior: A Study in Medicine, (1984) Carnegie Mellon University

@@texinfo:@anchor{Buchanan1984}@@ 10. Buchanan, Bruce G. and Shortliffe, Edward H., Rule Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project (The Addison-Wesley Series in Artificial Intelligence), (1984) Addison-Wesley Longman Publishing Co., Inc.

@@texinfo:@anchor{Steele77}@@ 11. Guy L. Steele Jr., Debunking the "expensive procedure call" myth or, procedure call implementations considered harmful or, {LAMBDA:} The Ultimate {GOTO, (1977) ACM

@@texinfo:@anchor{Steele1976}@@ 12. Steele Jr., Guy Lewis and Sussman, Gerald Jay, LAMBDA: the ultimate imperative, (1976) 

@@texinfo:@anchor{Moses1970}@@ 13. Moses, Joel, The Function of FUNCTION in LISP or Why the FUNARG Problem Should Be Called the Environment Problem, (1970) Association for Computing Machinery

@@texinfo:@anchor{IPE1984}@@ 14. Barstow, David R. and Shrobe, Howard E. and Sandewall, Erik., Interactive programming environments / editors, David R. Barstow, Howard E. Shrobe, Erik Sandewall, (1984) McGraw-Hill New York

@@texinfo:@anchor{Appel1991}@@ 15. Appel, Andrew W., Compiling with Continuations, (1991) Cambridge University Press

@@texinfo:@anchor{Bundy1984}@@ 16. Bundy, Alan and Wallen, Lincoln, Lispkit, (1984) Springer Berlin Heidelberg

@@texinfo:@anchor{Henderson1976}@@ 17. Henderson, Peter and Morris, James H., A Lazy Evaluator, (1976) Association for Computing Machinery

@@texinfo:@anchor{Henderson1980a}@@ 18. P. Henderson, Functional Programming - Application and Implementation, (1980) Prentice-Hall Int. Series in Computer Science

@@texinfo:@anchor{Henderson1980b}@@ 19. Peter Henderson, Functional programming - application and implementation, (1980) Prentice Hall

@@texinfo:@anchor{Traub1991}@@ 20. Kenneth R. Traub, Implementation of non-strict functional programming languages, (1991) Pitman

@@texinfo:@anchor{FriedmanWise1976}@@ 21. Daniel P. Friedman and
               David S. Wise, CONS} Should Not Evaluate its Arguments, (1976) Edinburgh University Press

@@texinfo:@anchor{Keller1979}@@ 22. R. M. {KELLER} and G. {LINDSTROM} and S. {PATIL, A loosely-coupled applicative multi-processing system*, (1979) 1979 International Workshop on Managing Requirements Knowledge (MARK)

@@texinfo:@anchor{Turner1979}@@ 23. D. Turner, A new implementation technique for applicative languages, (1979) Software: Practice and Experience

@@texinfo:@anchor{Landin1964}@@ 24. Landin, P. J., The Mechanical Evaluation of Expressions, (1964) The Computer Journal

* Index
  :PROPERTIES:
  :INDEX:    cp
  :END:

* Footnotes

[fn:28] See e.g. A Comparative Study Of Control Strategies For Expert Systems:
AGE Implementation Of Three Variations Of PUFF, /Nelleke Aiello/,  AAAI-83 Proceedings, (1983). And @@texinfo:@xref{Newell1972,,3}.@@ 

[fn:27] Merits of both interpretations are compared e.g. in The spineless tagless G-machine, /Simon L Peyton Jones, Jon Salkild/, Proceedings of the fourth international conference on Functional programming languages and computer architecture, pp. 184-201, (1989). 

[fn:26] We refer to the following for multiprocessing aspects of implementation: @@texinfo:@xref{Traub1991,,24},@@ @@texinfo:@xref{Keller1979,,26}.@@ See also: QLISP: A Language For The Interactive Development Of Complex Systems, /E. Sacerdoti, R. Fikes, R. Reboh, D. Sagalowicz, R. J. Waldinger, B. M. Wilber/, Artificial Intelligence Center, SRI, Technical Note 120, March 1976.

[fn:25] For more formal elaborations of the SECD machine see: Call-by-Name, Call-by-Value and the Lambda Calculus, /Gordon Plotkin/ (Theoretical Computer Science , Vol. 1, pp. 125-159, 1975); The Tail-Recursive SECD Machine, /John D. Ramsdell/, Journal of Automated Reasoning volume 23, p. 43–62 (1999); A Rational Deconstruction of Landin's SECD Machine, /Olivier Danvy/, In: Grelck C., Huch F., Michaelson G.J., Trinder P. (eds) Implementation and Application of Functional Languages. IFL 2004. Lecture Notes in Computer Science, vol 3474. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11431664_4 (2005); The SECD Microprocessor: A Verification Case Study, /Brian T. Graham/, The Springer International Series in Engineering and Computer Science (1992).

[fn:24] During the summer 1983, the authors visited Dr Peter Szolovits group at MIT and also met there with Dr. Ramesh Patil. 

[fn:23] See also: A LispKit Lisp Programming Language, a Modified Denotational Semantics Approach. /Jerinic, Ljubomir./ (1993); An experiment in practical semantics, /Maurice Naftalin/, ESOP 1986: ESOP 86 pp 144-159 (1986); A Compiler For Lispkit Targetted At Henderson's SECD Machine, /Simpson, T.; Birtwistle, G.; Hermann, M.; Graham, B./, Technical Report, University of Calgary (1989).

[fn:22] Parallel graph reduction with the (v , G)-machine. /Lennart Augustsson and Thomas Johnsson./ In Proceedings of the fourth international conference on Functional programming languages and computer architecture (FPCA 1989) DOI:https://doi.org/10.1145/99370.99386 

[fn:21] The abc-machine: A sequential stack-based abstract machine for graph rewriting, /Koopman, P. W. M., M. C. J. D. van Eekelen, E. G. J. M. H. Nocker, M. J. Plasmaijer and J. E. W. Smetsers/, Technical Report 90-22, Univ. Nijmegen, (1990).

[fn:20]  TIM: A simple, lazy abstract machine to execute supercombinatorics. /Fairbairn, J. and S. Wray./, FPCA (1987). 

[fn:19] The history of abstract machines for functional programming
languages is a fascinating subject of research. Cardelli’s Functional
Abstract Machine (1983) is a much extended and optimized SECD machine
used in the first native-code implementation of ML. The Categorical
Abstract Machine (1985) was developed by Cousineau et al.: its
instructions correspond to the constructions of a Cartesian closed
category, identity, composition, abstraction, application, pairing,
and selection. It was the base for the CAML implementation of ML. The
Zinc Abstract Machine (1990) developed by Leroy permits more efficient
execution. It is an optimized, strict version of the Krivine
machine. This machine is the basis of the bytecode versions of Leroy’s
Caml Light and Objective Caml implementations, and is used also in
Moscow ML.

[fn:18] A string processing language is a programming language that
focuses on string processing rather than processing numeric
data. String processing languages have been around for decades in the
form of command shells, programming tools, macro processors, and
scripting languages. This latter category has become prominent as
scripting language are used to ‘glue’ components together. There are 
myriads of scripting languages today.

[fn:17] Abstract machines for programming language implementation, /Stephan Diehl, Pieter Hartel, Peter Sestoft/, Future Generation Computer Systems 16 739–751, (2000).

[fn:16] From Operational Semantics to Abstract Machines, /John Hannan, Dale Miller/, (1992). 

[fn:15] Troubles with functionalism. /Ned Block/, Minnesota Studies in the Philosophy of Science, 9:261-325, (1978).

[fn:14] Not to mention neurosciences. See e.g. Toward discovery science of human brain function, /Bharat B. Biswal, et al./, Proceedings of the National Academy of Sciences Mar 2010, 107 (10) 4734-4739; (DOI: 10.1073/pnas.0911855107).

[fn:13] The causal approach was also characteristic of D.M. Armstrong's careful conceptual analysis of mental states and processes, such as perception and the secondary qualities, sensation, consciousness, belief, desire, emotion, voluntary action, in his A Materialist Theory of the Mind (1968).

[fn:12] Solutions to the mind/body problem usually try to answer questions such as: What is the ultimate nature of the mental? At the most general level, what makes a mental state mental? Or more specifically, What do thoughts have in common in virtue of which they are thoughts? That is, what makes a thought a thought? What makes a pain a pain? /Cartesian Dualism/ said the ultimate nature of the mental was to be found in a special mental substance. /Behaviorism/ identified mental states with behavioral dispositions; /physicalism/ in its most influential version identifies mental states with brain states. /Block, Ned/, What is Functionalism? The Encyclopedia of Philosophy Supplement, (1996).

[fn:11] Internist-I, an Experimental Computer-Based Diagnostic Consultant for General Internal Medicine, /Miller, Randolph A., Harry E. Pople Jr, and Jack D. Myers./ New England Journal of Medicine 307.8 (1982): 468-476. 

[fn:10] The order of the rules in the list attached to a goal is assumed to be
arbitrary, and all the rules are applied unless one of them succeeds and
concludes the value of the parameter with certainty (in which case the
remaining rules are superfluous).

[fn:9] EMYCIN : A Knowledge Engineer ’ s Tool for Constructing
Rule-Based Expert Systems, /William, Van Melle, E. Shortliffe,
Bruce. and G. Buchanan/, Pergamon-lnfotech state of the art report on
machine intelligence, pp. 249-263. Maidenhead, Berkshire, U.K.:
Infotech Ltd., 1981.

[fn:8] ONCOCIN: an expert system for oncology protocol management, /Edward H. Shortliffe, A. Carlisle Scott, Miriam B. Bischoff, A. Bruce Campbell, William Van Melle, and Charlotte D. Jacobs./ In Proceedings of the 7th international joint conference on Artificial intelligence - Volume 2 (IJCAI'81). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 876–881, 1981

[fn:7] Rete: A Fast Algorithm for the Many Pattern/Many Object Pattern
Match Problem, /Charles L. Forgy/, Artificial Intelligence 19, pp. 17-37, 1982.

[fn:6]  R1: An Expert in the Computer Systems Domain, /John McDermott/, Proceedings of the First AAAI Conference on Artificial Intelligence. AAAI'80. Stanford, California: AAAI Press: 269–271, 1980. (The paper won the AAAI Classic Paper Award in 1999.)

[fn:5] Thinking May Be More Than Computing, /Peter Kugel/, Cognition, 22 (1986) pp. 137-198. 

[fn:4] Designing a Rule System That Searches for Scientific Discoveries, /Douglas B. Lenat/ and /Gregory Harris/, CMU CS, Apr. 1977.

[fn:3]  In this context, progress in AI research was exemplified by  comprehensive descriptions of computer programs as landmark systems. @@texinfo:@xref{Feigenbaum1963,,5},@@ for such descriptions of the 1960s and 1950s systems. The import of the analogy between cognitive processes, in the human mind, and the workings of a computer program, was also felt in philosophy with the dramatic revival in the 1970s of /The Language of Thought/ hypothesis. The watershed was publication of Jerry Fodor's The Language of Thought (1975), triggering discussions and debates which continue to figure prominently within philosophy and cognitive science today. From a philosophical perspective, research programs such as "naturalizing intentionality" and "naturalizing consciousness" are still active although under a variety of cognitive assumptions, mirrorring the outpouring of results in neurosciences and computer science in the last decades.

[fn:2] The philosophical perspective so stated does not focus on /how/ the physical symbol system itself is actually realized. It obviously suggests that the human mind is such as system but also that, implicitly, physical symbol systems may be realised in Turing/Von Neumann computer architectures, and computer programs more specifically. And thus in the 1960s and 1970s it became an established practice to research theories about memory, reasoning or beliefs on computer programs as an experimental substrate, drawing conclusions deemed legitimate for cognition in the human mind. Contrast this to current Computational Neurosciences and to Connectionism's neural networks architectures, old and new.

[fn:1] Development of an expert system, /Daniel Sagalowicz/, Expert System, Vol. 1, Issue 2, Oct. 1984. 
