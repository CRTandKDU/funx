#+TITLE: A Functional Perspective on the NXP Architecture
#+SUBTITLE: Version {{{version}}}
#+AUTHOR: jmc
#+DATE: <2021-01-11 lun.>
#+OPTIONS: ':t toc:t author:t
#+LANGUAGE: en

#+MACRO: version 1.0

#+TEXINFO_FILENAME: funxp.info
#+TEXINFO_HEADER: @syncodeindex fn cp

#+TEXINFO_DIR_CATEGORY: NXP Architecture
#+TEXINFO_DIR_TITLE: funxp: (funxp)
#+TEXINFO_DIR_DESC: A Functional NXP Architecture

#+TEXINFO_PRINTED_TITLE: FUNXP

This manual is for FUNXP, a functional perspective on the NXP Architecture (version {{{version}}}).

* Introduction
Most of the ideas expressed in the historical series of computer programs, termed /NXP Architecture/ in this paper, were originally designed at the apex of one of the recurring cycle in the 30-year returns of the Artificial Intelligence (AI) debate, @@texinfo:@xref{JMC2018,,1}.@@ 

** A Dominant View, Symbolic Artificial Intelligence
#+CINDEX: Symbolic AI
Progressively in the late 70s and early 80s, the symbolic thread triumphed as the dominant paradigm of AI research. In 1984 for instance, at the climax of this cycle, early efforts to transfer research work to industrial applications were so introduced:

#+BEGIN_QUOTE
Artificial Intelligence is the subfield of computer science concerned with symbolic reasoning and problem-solving. [...] Knowledge Engineering is the process of incorporating symbolic knowledge into computer systems to solve problems normally requiring human attention and intelligence.[fn:1]
#+END_QUOTE 

#+CINDEX: Expert System
These computer systems incorporating symbolic knowledge became known as /Expert Systems/ and were all the rage, as industrial applications of AI, from the early eighties to the mid-nineties.

This somewhat restricted, to contemporary eyes, view of AI as a "subfield of computer science" geared towards augmenting traditional programs with symbolic knowledge was, however, perfectly in line with the precepts of the symbolic thread of AI research, established at the /Dartmouth Summer Research Project on Artificial Intelligence/ in 1956. Out of the variety of conceptual tenets of research on "thinking machines" in the 1950s, diversely known as /cybernetics/, /automata theory/ or /complex information processing/, stemmed out the original characteristics of the symbolic approach to AI: the importance of (mathematical) logic in high-level cognitive processes, and the prevalence of symbols in all aspects of cognition.

#+CINDEX: Physical Symbol System Hypothesis
The defining assumption, /the physical symbol system hypothesis/, is the philosophical perspective on AI assessed by Allen Newell and Herbert A. Simon:

#+BEGIN_QUOTE
A physical symbol system has the necessary and sufficient means for general intelligent action, @@texinfo:@xref{Newell1972,,3}.@@
#+END_QUOTE 

#+CINDEX: goal
#+CINDEX: working memory
#+CINDEX: computer/brain analogy
With symbols, structures come naturally as list of symbols and cognitive processes are understood as controlled by signal and symbol structures in /working memory/, i.e. arbitrary list structures in an extensive database-like system, called /goals/, @@texinfo:@xref{PDIS1978,,4}.@@ On the one hand, note the implicit analogy with the Turing machine model of a computer[fn:2], where perceptions are turned into symbols in memory, feeding a processor which elects further actions, operating changes in the environment. And, on the other hand, delineating further this analogy, the notions of symbols and lists from which LISP, and all its functional programming languages descendance in computer science, was derived, by the same contributors to the seminal Dartmouth workshop[fn:3].

#+CINDEX: CMU
While the seminal ideas for the NXP Architecture stemmed from a very early interest in what would be now termed /learning concepts/ (see [[How we got here]]), they were readily influenced and refined by the dominant view at the time, all the more so that they matured in the cultural environment of Carnegie-Mellon University (CMU), namely of its Computer Science and Robotics Institute departments, from 1982 to 1984.

** Production Systems and Rule-based Systems.
#+CINDEX: rule-based system
#+CINDEX: production system
With the symbolic AI view now firmly entrenched, both in computer science and in the emerging philosophy of the mind, the idea of rule-based systems, in which /knowledge/ is conventionally represented as /rules/ operating on symbols (following the Physical Symbol System hypothesis) became an active applied research program.

#+CAPTION: Model Human Processor in /The Psychology of Human-computer Interaction/, by Card, Newell, Moran (1983). The brain-as-a-computer metaphor in the philosophy of the mind, all quantified!
#+attr_texinfo: :width 200px :center t
[[./MODEL-HUMAN-PROCESSOR-w200.jpg]]

Although rule-based computation was originally used for formal and systems purposes , researchers in Artificial Intelligence (AI) found that the same methodology was also useful for modelling a wide variety of sophisticated tasks.

#+BEGIN_QUOTE
The production system was one of those happy events, though in minor key, that historians of science often talk about: a rather well-prepared formalism, sitting in wait for a scientific mission. Production systems have a long and diverse history. Their use in symbolic logic starts with Post, @@texinfo:@xref{Post1943,,7},@@ from whom the name is taken. They also show up as Markov algorithms, @@texinfo:@xref{Markov1957,,8}.@@ Their use in linguistics, where they are also called rewrite rules, dates from Chomsky, @@texinfo:@xref{Chomsky57,,10}.@@ As with many other notions in computer science, they really entered into wide currency when they became operationalized in programming languages, first in string manipulation systems and in compiler translation languages, @@texinfo:@xref{Floyd1961,,9}.@@ Thus they were at hand when the data on human problem solving finally took a form (Problem Behavior Graphs) that pointed to their usefulness, @@texinfo:@xref{Newell1972,,3}.@@
#+END_QUOTE

As commented on by Lenat[fn:4], there were many design constraints present in the classical formal rule based systems. Many of these details were preserved in the AI production rule based programs (e.g., forcing all state information into a single string of tokens). But there were also many changes. The whole notion of "what a rule system really is" changed from an effective problem statement to a tendency to solve problems in a particular way. One typical corollary of this change of view was that instead of no external inputs whatsoever, there was now a presumption of some "environment" which supplied new entries into the token sequence.

#+CINDEX: rule-based-system
#+CINDEX: working memory
#+CINDEX: inference engine
#+CINDEX: recognize-act-cycle
So over the 1970s symbolic AI research mostly worked with rule systems (RS), a collection of condition-action rules, together with associated data structures (DS; also called memories, or /working memory/) which the rules may inspect and alter. There must also be a policy for /interpretation/: detecting and firing relevant rules (also known as /recognize-act-cycle/, later called /inference engine/). "Intelligence is ten million rules," [[https://www.wired.com/2016/03/doug-lenat-artificial-intelligence-common-sense-engine/][pronounced Lenat in 1988]], such were the times.

*** Neo-classical Rule System Architecture
#+CINDEX: LHS
#+CINDEX: RHS
#+CINDEX: working memory
#+CINDEX: conditions
#+CINDEX: actions
In ten loose principles:

#+ATTR_TEXINFO: :table-type vtable :sep , :indic asis
- Principle of Simple Memories :: One or two uniform data structures define sufficient memories for a rule system to read from and write into. The format for entries In these structures is both uncomplicated and unchanging.
- Principle of Simple DS Accesses :: The primitive read and write operations are as simple and low-level as possible; typically they are simply a membership or equality test type of read, and an insert-new-element or set-value type of write. More complicated, algorithmic operations on the memories are not available to the rules.
- Principle of Isolated DS Elements :: Elements of the uniform DS cannot point to (parts of) other elements. This follows from the preceding principle: If we aren't allowed to chase pointers, there may as well not be any.
- Principle of Continuous Attention :: In addition to the one or two simple data structures, there may be an external environment which continuously inserts stimuli into the DS. The interleaving of stimuli and internally generated symbols is managed quite trivially: (a) The stimuli are simply inserted into the DS as new or changed elements; (b) Each rule is so small and quick that no "interruption" mechanism is necessary. The interpreter may ignore any suddenly-added stimulus until the current rule finishes executing. The RS may be viewed as "continuously" attending to the environment.
- Principle of Opaque Rules :: Rules need not have a format inspectable by other rules, but rather can be coded in whatever way is convenient for the programmer and the rule interpreter; i.e., the set of rules is not treated as one of the RSs data structures. E.g., the condition parts of rules may be barred from fully analyzing the set of productions, and the action parts of rules may not be allowed to operate on existing rules.
- Principle of Simple Rules :: Rules consist of a left- and a right-hand side which are quite elementary. The left hand side (lhs, situation characterization, IF-part, condition) is typically a pattern-match composed with a primitive DS read access, and the right hand side (rhs, consequence, THEN-part, action) is also simply a primitive DS write access. There is no need for sophisticated bundles of DS accesses on either side of a rule. Thus several extra rules should be preferred to a single rule with several actions.
- Principle of Encoding by Coupled Rules :: A collection of interrelated rules is used to accomplish each subtask; i.e., wherever a subroutine would be used in a procedural programming language. For example, programming an iteration may require many rules "coupled" by writing and reading special (Le., otherwise meaningless) loop control notes in the data structure. 
-  Principle of Knowledge as Rules :: All knowledge of substance should be, can : be, and is represented as rules. This includes all non-trivial domain dependent information. The role of the DS is just to hold simple descriptive information, intermediate control state messages, recent stimuli from the environment, etc.
- Principle of Simple Interpretation :: The topmost control flow in the RS is via a simple rule interpreter. After a rule fires, it is essential that any rule in the system may potentially be the next one to fire (i.e., it is forbidden to locate a set of relevant rules and fire them off in sequence). When the rhs of a rule is executed, it can (and frequently will) drastically alter the situation that determined which rules were relevant.
- Principle of Closure :: The representations allowed by (1-9) are sufficient and appropriate for organizing all the kinds of knowledge needed for tasks for which a given RS is designed.

Notice the common theme: the adequacy of simplicity in all dimensions. 

Medical consultation as a task environment.

* How we got here

A quick word on previous implementations. Interpreter v. compilers.

Emacs-Lisp.

~funxp~ is both a programming language and an interactive programming environment for expert systems. (Ref IPE1984.) It embeds NCLOSE, an earlier inference engine, in a simple -- even simplistic -- functional programming language called ~funx~ (/functional expressions/).

* A simplistic functional language, funx
#+CINDEX: funx
This section describes a simple functional programming language, ~funx~, evidently based on LISP with a minimal set of built-in functions. 

** Rehash of Henderson's book and other references of the 60s and 70s. Sexps.

** Simple subset of LISP. SECD Machine. Compiling funx to SECD assembly.

* Extending for NXP-style inferencing, funxp
#+CINDEX: funxp
Promises and delay/force. A mention of thread and parallelism (QLISP, Kugel non-halting computations v. thinking).

Data-driven vs. call: Hewitt mentioned in Sacerdoti as pattern-directed function invocation.

NXP-style rules. Rule: Hypo LHS &optional RHS &optional :context.

Glossary of terms: hypo(thesis), cond(itions), LHS/RHS, actions, sign, goal/subgoal, backward/forward chaining, knowcess, gating...

* Compiling funxp to SECD assembly

Knowledge base, or rule sets, are compiled to funxp environments.

Decorations and globales.

* An Emacs-based client

** Session. Interactivity. Trace and protocol.

** Encyclopedia and tree representation. Commands.
#+CINDEX: Encylopedia

#+ATTR_TEXINFO: :table-type vtable 
#+BEGIN_QUOTE
  - `q' :: Kill Encyclopedia buffer.
  - `k' :: Suggest hypo at point and knowcess.
  - `w' :: Volunteer, or What-if, data at point and knowcess.
  - `a' :: Answer pending question and resume session.
  - `r' :: Restart session.
  - `t' :: Open backward-chaining tree of hypo at point.
#+END_QUOTE

* Bibliography
Source: ~funx.bib~.

#+NAME: bibliography
#+BEGIN_SRC emacs-lisp :results value raw :exports results 
  (require 'parsebib)
  (require 'subr-x)

  (defun funx-parse (fname)
    (with-temp-buffer
      (insert-file-contents fname)
      (parsebib-collect-entries)))

  (defun funx-trim (str)
    (let ((re "[ \t\n\r\"{}]+"))
      (string-trim-left (string-trim-right str re) re)))

  (defun funx-first (keys alist)
    (if (null keys) ""
      (if (assoc (car keys) alist)
	  (cdr (assoc (car keys) alist))
	(funx-first (cdr keys) alist))))

  (let ((nref 0)
	(outstr "\n\n")
	(funx-bib (funx-parse "C:/Users/jmc/Documents/code/funx/funx.bib")))
    (maphash
     #'(lambda (key value)
	 (setq nref (1+ nref))
	 (setq outstr
	       (concat
		outstr
		(format
		 "@@texinfo:@anchor{%s}@@%d. %s. /%s/. %s, %s.\n\n"
		 key nref
		 (funx-trim (cdr (assoc "author" value)))
		 (funx-trim (cdr (assoc "title"  value)))
		 (funx-trim
		  (funx-first '("publisher" "journal" "institution") value))
		 (funx-trim (cdr (assoc "year"   value))))
		)))
     funx-bib)
     outstr)
#+END_SRC
* Test WIP                                                         :noexport:

From funx.bib

@@texinfo:@anchor{Rougier2005}@@ 1. Rougier, Nicolas P. and Noelle, David C. and Braver, Todd S. and Cohen, Jonathan D. and O{\textquoteright}Reilly, Randall C., Prefrontal cortex and flexible cognitive control: Rules without symbols, (2005) National Academy of Sciences

@@texinfo:@anchor{Newell1972}@@ 2. Newell, Allen and Simon, H. A., Human Problem Solving, (1972) Prentice-Hall, Inc.

@@texinfo:@anchor{PDIS1978}@@ 3. Waterman, D. A. and Hayes-Roth, Frederick, Pattern-Directed Inference Systems, (1978) Academic Press, Inc.

@@texinfo:@anchor{Post1943}@@ 4. Emil L. Post, Formal Reductions of the General Combinatorial Decision Problem, (1943) Association for Symbolic Logic

@@texinfo:@anchor{Markov1957}@@ 5. A. A. Markov, Theory of Algorithms, (1957) Association for Symbolic Logic

@@texinfo:@anchor{Floyd1961}@@ 6. Floyd, Robert W., An Algorithm for Coding Efficient Arithmetic Operations, (1961) Association for Computing Machinery

@@texinfo:@anchor{Chomsky57}@@ 7. Chomsky, Noam, Syntactic Structures, (1957) Mouton and Co.

@@texinfo:@anchor{Paycha1963}@@ 8. Paycha, F., Cybern{\'e}tique de la consultation: logique et morale de la m{\'e}decine, (1963) Gauthier-Villars

@@texinfo:@anchor{Rappaport-1984-15190}@@ 9. Alain Rappaport and Jean-Marie C. Chauvet, Symbolic Knowledge Processing for he Acquisition of Expert Behavior: A Study in Medicine, (1984) Carnegie Mellon University

@@texinfo:@anchor{Buchanan1984}@@ 10. Buchanan, Bruce G. and Shortliffe, Edward H., Rule Based Expert Systems: The Mycin Experiments of the Stanford Heuristic Programming Project (The Addison-Wesley Series in Artificial Intelligence), (1984) Addison-Wesley Longman Publishing Co., Inc.

@@texinfo:@anchor{Steele77}@@ 11. Guy L. Steele Jr., Debunking the "expensive procedure call" myth or, procedure call implementations considered harmful or, {LAMBDA:} The Ultimate {GOTO, (1977) ACM

@@texinfo:@anchor{Steele1976}@@ 12. Steele Jr., Guy Lewis and Sussman, Gerald Jay, LAMBDA: the ultimate imperative, (1976) 

@@texinfo:@anchor{Moses1970}@@ 13. Moses, Joel, The Function of FUNCTION in LISP or Why the FUNARG Problem Should Be Called the Environment Problem, (1970) Association for Computing Machinery

@@texinfo:@anchor{IPE1984}@@ 14. Barstow, David R. and Shrobe, Howard E. and Sandewall, Erik., Interactive programming environments / editors, David R. Barstow, Howard E. Shrobe, Erik Sandewall, (1984) McGraw-Hill New York

@@texinfo:@anchor{Appel1991}@@ 15. Appel, Andrew W., Compiling with Continuations, (1991) Cambridge University Press

@@texinfo:@anchor{Bundy1984}@@ 16. Bundy, Alan and Wallen, Lincoln, Lispkit, (1984) Springer Berlin Heidelberg

@@texinfo:@anchor{Henderson1976}@@ 17. Henderson, Peter and Morris, James H., A Lazy Evaluator, (1976) Association for Computing Machinery

@@texinfo:@anchor{Henderson1980a}@@ 18. P. Henderson, Functional Programming - Application and Implementation, (1980) Prentice-Hall Int. Series in Computer Science

@@texinfo:@anchor{Henderson1980b}@@ 19. Peter Henderson, Functional programming - application and implementation, (1980) Prentice Hall

@@texinfo:@anchor{Traub1991}@@ 20. Kenneth R. Traub, Implementation of non-strict functional programming languages, (1991) Pitman

@@texinfo:@anchor{FriedmanWise1976}@@ 21. Daniel P. Friedman and
               David S. Wise, CONS} Should Not Evaluate its Arguments, (1976) Edinburgh University Press

@@texinfo:@anchor{Keller1979}@@ 22. R. M. {KELLER} and G. {LINDSTROM} and S. {PATIL, A loosely-coupled applicative multi-processing system*, (1979) 1979 International Workshop on Managing Requirements Knowledge (MARK)

@@texinfo:@anchor{Turner1979}@@ 23. D. Turner, A new implementation technique for applicative languages, (1979) Software: Practice and Experience

@@texinfo:@anchor{Landin1964}@@ 24. Landin, P. J., The Mechanical Evaluation of Expressions, (1964) The Computer Journal

* Index
  :PROPERTIES:
  :INDEX:    cp
  :END:

* Footnotes

[fn:4] Designing a Rule System That Searches for Scientific Discoveries, /Douglas B. Lenat/ and /Gregory Harris/, CMU CS, Apr. 1977.

[fn:3]  In this context, progress in AI research was exemplified by  comprehensive descriptions of computer programs as landmark systems. @@texinfo:@xref{Feigenbaum1963,,5},@@ for such descriptions of the 1960s and 1950s systems. The import of the analogy between cognitive processes, in the human mind, and the workings of a computer program, was also felt in philosophy with the dramatic revival in the 1970s of /The Language of Thought/ hypothesis. The watershed was publication of Jerry Fodor's The Language of Thought (1975), triggering discussions and debates which continue to figure prominently within philosophy and cognitive science today. From a philosophical perspective, research programs such as "naturalizing intentionality" and "naturalizing consciousness" are still active although under a variety of cognitive assumptions, mirrorring the outpouring of results in neurosciences and computer science in the last decades.

[fn:2] The philosophical perspective so stated does not focus on /how/ the physical symbol system itself is actually realized. It obviously suggests that the human mind is such as system but also that, implicitly, physical symbol systems may be realised in Turing/Von Neumann computer architectures, and computer programs more specifically. And thus in the 1960s and 1970s it became an established practice to research theories about memory, reasoning or beliefs on computer programs as an experimental substrate, drawing conclusions deemed legitimate for cognition in the human mind. Contrast this to current Computational Neurosciences and to Connectionism's neural networks architectures, old and new.

[fn:1] Development of an expert system, /Daniel Sagalowicz/, Expert System, Vol. 1, Issue 2, Oct. 1984. 
